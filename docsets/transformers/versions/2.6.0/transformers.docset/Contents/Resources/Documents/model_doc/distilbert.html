
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>DistilBERT — transformers 2.6.0 documentation</title>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<script src="../_static/js/modernizr.min.js" type="text/javascript"></script>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/language_data.js"></script>
<script src="../_static/js/custom.js"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="ctrl.html" rel="next" title="CTRL"/>
<link href="roberta.html" rel="prev" title="RoBERTa"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model upload and sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchscript.html">TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">DistilBERT</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#distilbertconfig">DistilBertConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distilberttokenizer">DistilBertTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distilbertmodel">DistilBertModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distilbertformaskedlm">DistilBertForMaskedLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distilbertforsequenceclassification">DistilBertForSequenceClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distilbertforquestionanswering">DistilBertForQuestionAnswering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfdistilbertmodel">TFDistilBertModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfdistilbertformaskedlm">TFDistilBertForMaskedLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfdistilbertforsequenceclassification">TFDistilBertForSequenceClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfdistilbertforquestionanswering">TFDistilBertForQuestionAnswering</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a href="../index.html">Docs</a> »</li>
<li>DistilBERT</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/model_doc/distilbert.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="distilbert">
<h1>DistilBERT<a class="headerlink" href="#distilbert" title="Permalink to this headline">¶</a></h1>
<p>The DistilBERT model was proposed in the blog post
<a class="reference external" href="https://medium.com/huggingface/distilbert-8cf3380435b5">Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT</a>,
and the paper <a class="reference external" href="https://arxiv.org/abs/1910.01108">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>.
DistilBERT is a small, fast, cheap and light Transformer model trained by distilling Bert base. It has 40% less
parameters than <cite>bert-base-uncased</cite>, runs 60% faster while preserving over 95% of Bert’s performances as measured on
the GLUE language understanding benchmark.</p>
<p>The abstract from the paper is the following:</p>
<p><em>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we
leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a
BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage
the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language
modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train
and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative
on-device study.</em></p>
<p>Tips:</p>
<ul class="simple">
<li><p>DistilBert doesn’t have <cite>token_type_ids</cite>, you don’t need to indicate which token belongs to which segment. Just separate your segments with the separation token <cite>tokenizer.sep_token</cite> (or <cite>[SEP]</cite>)</p></li>
<li><p>DistilBert doesn’t have options to select the input positions (<cite>position_ids</cite> input). This could be added if necessary though, just let’s us know if you need this option.</p></li>
</ul>
<div class="section" id="distilbertconfig">
<h2>DistilBertConfig<a class="headerlink" href="#distilbertconfig" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.DistilBertConfig"><a name="//apple_ref/cpp/Class/transformers.DistilBertConfig"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DistilBertConfig</code><span class="sig-paren">(</span><em class="sig-param">vocab_size=30522</em>, <em class="sig-param">max_position_embeddings=512</em>, <em class="sig-param">sinusoidal_pos_embds=False</em>, <em class="sig-param">n_layers=6</em>, <em class="sig-param">n_heads=12</em>, <em class="sig-param">dim=768</em>, <em class="sig-param">hidden_dim=3072</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">attention_dropout=0.1</em>, <em class="sig-param">activation='gelu'</em>, <em class="sig-param">initializer_range=0.02</em>, <em class="sig-param">qa_dropout=0.1</em>, <em class="sig-param">seq_classif_dropout=0.2</em>, <em class="sig-param">pad_token_id=0</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/configuration_distilbert.html#DistilBertConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the configuration class to store the configuration of a <a class="reference internal" href="#transformers.DistilBertModel" title="transformers.DistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertModel</span></code></a>.
It is used to instantiate a DistilBERT model according to the specified arguments, defining the model
architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of
the DistilBERT <a class="reference external" href="https://huggingface.co/distilbert-base-uncased">distilbert-base-uncased</a> architecture.</p>
<p>Configuration objects inherit from  <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> and can be used
to control the model outputs. Read the documentation from  <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>
for more information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 30522) – Vocabulary size of the DistilBERT model. Defines the different tokens that
can be represented by the <cite>inputs_ids</cite> passed to the forward method of <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a>.</p></li>
<li><p><strong>max_position_embeddings</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 512) – The maximum sequence length that this model might ever be used with.
Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</p></li>
<li><p><strong>sinusoidal_pos_embds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to use sinusoidal positional embeddings.</p></li>
<li><p><strong>n_layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 6) – Number of hidden layers in the Transformer encoder.</p></li>
<li><p><strong>n_heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 12) – Number of attention heads for each attention layer in the Transformer encoder.</p></li>
<li><p><strong>dim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 768) – Dimensionality of the encoder layers and the pooler layer.</p></li>
<li><p><strong>hidden_dim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 3072) – The size of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.</p></li>
<li><p><strong>dropout</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.1) – The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.</p></li>
<li><p><strong>attention_dropout</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.1) – The dropout ratio for the attention probabilities.</p></li>
<li><p><strong>activation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">function</span></code>, optional, defaults to “gelu”) – The non-linear activation function (function or string) in the encoder and pooler.
If string, “gelu”, “relu”, “swish” and “gelu_new” are supported.</p></li>
<li><p><strong>initializer_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.02) – The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p></li>
<li><p><strong>qa_dropout</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.1) – The dropout probabilities used in the question answering model
<code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForQuestionAnswering</span></code>.</p></li>
<li><p><strong>seq_classif_dropout</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.2) – The dropout probabilities used in the sequence classification model
<code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertModel</span><span class="p">,</span> <span class="n">DistilBertConfig</span>

<span class="c1"># Initializing a DistilBERT configuration</span>
<span class="n">configuration</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="p">()</span>

<span class="c1"># Initializing a model from the configuration</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>

<span class="c1"># Accessing the model configuration</span>
<span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</pre></div>
</div>
<dl class="attribute">
<dt id="transformers.DistilBertConfig.pretrained_config_archive_map"><a name="//apple_ref/cpp/Attribute/transformers.DistilBertConfig.pretrained_config_archive_map"></a>
<code class="sig-name descname">pretrained_config_archive_map</code><a class="headerlink" href="#transformers.DistilBertConfig.pretrained_config_archive_map" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing all the available pre-trained checkpoints.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Dict[str, str]</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="distilberttokenizer">
<h2>DistilBertTokenizer<a class="headerlink" href="#distilberttokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.DistilBertTokenizer"><a name="//apple_ref/cpp/Class/transformers.DistilBertTokenizer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DistilBertTokenizer</code><span class="sig-paren">(</span><em class="sig-param">vocab_file</em>, <em class="sig-param">do_lower_case=True</em>, <em class="sig-param">do_basic_tokenize=True</em>, <em class="sig-param">never_split=None</em>, <em class="sig-param">unk_token='[UNK]'</em>, <em class="sig-param">sep_token='[SEP]'</em>, <em class="sig-param">pad_token='[PAD]'</em>, <em class="sig-param">cls_token='[CLS]'</em>, <em class="sig-param">mask_token='[MASK]'</em>, <em class="sig-param">tokenize_chinese_chars=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_distilbert.html#DistilBertTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a DistilBertTokenizer.
<a class="reference internal" href="#transformers.DistilBertTokenizer" title="transformers.DistilBertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertTokenizer</span></code></a> is identical to <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> and runs end-to-end
tokenization: punctuation splitting + wordpiece.</p>
<p>Refer to superclass <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> for usage examples and documentation concerning
parameters.</p>
</dd></dl>
</div>
<div class="section" id="distilbertmodel">
<h2>DistilBertModel<a class="headerlink" href="#distilbertmodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.DistilBertModel"><a name="//apple_ref/cpp/Class/transformers.DistilBertModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DistilBertModel</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertModel" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top.</p>
<p>This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</p>
</dd>
</dl>
<dl class="method">
<dt id="transformers.DistilBertModel.forward"><a name="//apple_ref/cpp/Method/transformers.DistilBertModel.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input_ids=None</em>, <em class="sig-param">attention_mask=None</em>, <em class="sig-param">head_mask=None</em>, <em class="sig-param">inputs_embeds=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.DistilBertModel" title="transformers.DistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertModel</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.DistilBertTokenizer" title="transformers.DistilBertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.DistilBertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl>
<dt>last_hidden_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>):</dt><dd><p>Sequence of hidden-states at the output of the last layer of the model.</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt><dd><p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt><dd><p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) and inputs</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertModel</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"Hello, my dog is cute"</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

<span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># The last hidden-state is the first element of the output tuple</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.DistilBertModel.get_input_embeddings"><a name="//apple_ref/cpp/Method/transformers.DistilBertModel.get_input_embeddings"></a>
<code class="sig-name descname">get_input_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertModel.get_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertModel.get_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s input embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping vocabulary to hidden states.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.DistilBertModel.set_input_embeddings"><a name="//apple_ref/cpp/Method/transformers.DistilBertModel.set_input_embeddings"></a>
<code class="sig-name descname">set_input_embeddings</code><span class="sig-paren">(</span><em class="sig-param">new_embeddings</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertModel.set_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertModel.set_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Set model’s input embeddings</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) – A module mapping vocabulary to hidden states.</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="distilbertformaskedlm">
<h2>DistilBertForMaskedLM<a class="headerlink" href="#distilbertformaskedlm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.DistilBertForMaskedLM"><a name="//apple_ref/cpp/Class/transformers.DistilBertForMaskedLM"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DistilBertForMaskedLM</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertForMaskedLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertForMaskedLM" title="Permalink to this definition">¶</a></dt>
<dd><p>DistilBert Model with a <cite>masked language modeling</cite> head on top.</p>
<p>This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</p>
</dd>
</dl>
<dl class="method">
<dt id="transformers.DistilBertForMaskedLM.forward"><a name="//apple_ref/cpp/Method/transformers.DistilBertForMaskedLM.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input_ids=None</em>, <em class="sig-param">attention_mask=None</em>, <em class="sig-param">head_mask=None</em>, <em class="sig-param">inputs_embeds=None</em>, <em class="sig-param">masked_lm_labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertForMaskedLM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertForMaskedLM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.DistilBertTokenizer" title="transformers.DistilBertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.DistilBertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>masked_lm_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the masked language modeling loss.
Indices should be in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code> (see <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> docstring)
Tokens with indices set to <code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored (masked), the loss is only computed for the tokens with labels
in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl>
<dt>loss (<cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">masked_lm_labels</span></code> is provided) <code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(1,)</span></code>:</dt><dd><p>Masked language modeling loss.</p>
</dd>
<dt>prediction_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>)</dt><dd><p>Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt><dd><p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt><dd><p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) and inputs</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertForMaskedLM</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"Hello, my dog is cute"</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">masked_lm_labels</span><span class="o">=</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">prediction_scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.DistilBertForMaskedLM.get_output_embeddings"><a name="//apple_ref/cpp/Method/transformers.DistilBertForMaskedLM.get_output_embeddings"></a>
<code class="sig-name descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertForMaskedLM.get_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertForMaskedLM.get_output_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s output embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping hidden states to vocabulary.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="distilbertforsequenceclassification">
<h2>DistilBertForSequenceClassification<a class="headerlink" href="#distilbertforsequenceclassification" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.DistilBertForSequenceClassification"><a name="//apple_ref/cpp/Class/transformers.DistilBertForSequenceClassification"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DistilBertForSequenceClassification</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertForSequenceClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertForSequenceClassification" title="Permalink to this definition">¶</a></dt>
<dd><p>DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of
the pooled output) e.g. for GLUE tasks.</p>
<p>This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</p>
</dd>
</dl>
<dl class="method">
<dt id="transformers.DistilBertForSequenceClassification.forward"><a name="//apple_ref/cpp/Method/transformers.DistilBertForSequenceClassification.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input_ids=None</em>, <em class="sig-param">attention_mask=None</em>, <em class="sig-param">head_mask=None</em>, <em class="sig-param">inputs_embeds=None</em>, <em class="sig-param">labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertForSequenceClassification.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertForSequenceClassification.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.DistilBertForSequenceClassification" title="transformers.DistilBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.DistilBertTokenizer" title="transformers.DistilBertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.DistilBertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the sequence classification/regression loss.
Indices should be in <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl>
<dt>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">label</span></code> is provided):</dt><dd><p>Classification (or regression if config.num_labels==1) loss.</p>
</dd>
<dt>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.num_labels)</span></code>):</dt><dd><p>Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt><dd><p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt><dd><p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) and inputs</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertForSequenceClassification</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"Hello, my dog is cute"</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="distilbertforquestionanswering">
<h2>DistilBertForQuestionAnswering<a class="headerlink" href="#distilbertforquestionanswering" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.DistilBertForQuestionAnswering"><a name="//apple_ref/cpp/Class/transformers.DistilBertForQuestionAnswering"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DistilBertForQuestionAnswering</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertForQuestionAnswering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertForQuestionAnswering" title="Permalink to this definition">¶</a></dt>
<dd><p>DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of
the hidden-states output to compute <cite>span start logits</cite> and <cite>span end logits</cite>).</p>
<p>This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</p>
</dd>
</dl>
<dl class="method">
<dt id="transformers.DistilBertForQuestionAnswering.forward"><a name="//apple_ref/cpp/Method/transformers.DistilBertForQuestionAnswering.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input_ids=None</em>, <em class="sig-param">attention_mask=None</em>, <em class="sig-param">head_mask=None</em>, <em class="sig-param">inputs_embeds=None</em>, <em class="sig-param">start_positions=None</em>, <em class="sig-param">end_positions=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_distilbert.html#DistilBertForQuestionAnswering.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DistilBertForQuestionAnswering.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.DistilBertForQuestionAnswering" title="transformers.DistilBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForQuestionAnswering</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.DistilBertTokenizer" title="transformers.DistilBertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.DistilBertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>start_positions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<cite>sequence_length</cite>).
Position outside of the sequence are not taken into account for computing the loss.</p></li>
<li><p><strong>end_positions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<cite>sequence_length</cite>).
Position outside of the sequence are not taken into account for computing the loss.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl>
<dt>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> is provided):</dt><dd><p>Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</dd>
<dt>start_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,)</span></code>):</dt><dd><p>Span-start scores (before SoftMax).</p>
</dd>
<dt>end_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,)</span></code>):</dt><dd><p>Span-end scores (before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt><dd><p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt><dd><p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) and inputs</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertForQuestionAnswering</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"Hello, my dog is cute"</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">start_positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">end_positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">])</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">start_positions</span><span class="o">=</span><span class="n">start_positions</span><span class="p">,</span> <span class="n">end_positions</span><span class="o">=</span><span class="n">end_positions</span><span class="p">)</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">start_scores</span><span class="p">,</span> <span class="n">end_scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfdistilbertmodel">
<h2>TFDistilBertModel<a class="headerlink" href="#tfdistilbertmodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.TFDistilBertModel"><a name="//apple_ref/cpp/Class/transformers.TFDistilBertModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFDistilBertModel</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">*inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_distilbert.html#TFDistilBertModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDistilBertModel" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare DistilBERT encoder/transformer outputing raw hidden-states without any specific head on top.
This model is a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model">tf.keras.Model</a> sub-class.
Use it as a regular TF 2.0 Keras Model and
refer to the TF 2.0 documentation for all matter related to general usage and behavior.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>TF 2.0 models accepts two formats as inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>having all inputs as keyword arguments (like PyTorch models), or</p></li>
<li><p>having all inputs as a list, tuple or dict in the first positional arguments.</p></li>
</ul>
</div></blockquote>
<p>This second option is useful when using <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.Model.fit()</span></code> method which currently requires having
all the tensors in the first argument of the model call function: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs)</span></code>.</p>
<p>If you choose this second option, there are three possibilities you can use to gather all the input Tensors
in the first positional argument :</p>
<ul class="simple">
<li><p>a single Tensor with input_ids only and nothing else: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs_ids)</span></code></p></li>
<li><p>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask])</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">token_type_ids])</span></code></p></li>
<li><p>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model({'input_ids':</span> <span class="pre">input_ids,</span> <span class="pre">'token_type_ids':</span> <span class="pre">token_type_ids})</span></code></p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</p>
</dd>
</dl>
<dl class="method">
<dt id="transformers.TFDistilBertModel.call"><a name="//apple_ref/cpp/Method/transformers.TFDistilBertModel.call"></a>
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_distilbert.html#TFDistilBertModel.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDistilBertModel.call" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TFDistilBertModel" title="transformers.TFDistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertModel</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">embedding_dim)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>training</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to activate dropout modules (if set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) during training or to de-activate them
(if set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) for evaluation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl>
<dt>last_hidden_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>):</dt><dd><p>Sequence of hidden-states at the output of the last layer of the model.</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt><dd><p>tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt><dd><p>tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>:</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<code class="xref py py-class docutils literal notranslate"><span class="pre">transformers,DistilBertConfig</span></code>) and inputs</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">TFDistilBertModel</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"Hello, my dog is cute"</span><span class="p">))[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># The last hidden-state is the first element of the output tuple</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfdistilbertformaskedlm">
<h2>TFDistilBertForMaskedLM<a class="headerlink" href="#tfdistilbertformaskedlm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.TFDistilBertForMaskedLM"><a name="//apple_ref/cpp/Class/transformers.TFDistilBertForMaskedLM"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFDistilBertForMaskedLM</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">*inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_distilbert.html#TFDistilBertForMaskedLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDistilBertForMaskedLM" title="Permalink to this definition">¶</a></dt>
<dd><p>DistilBert Model with a <cite>masked language modeling</cite> head on top.
This model is a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model">tf.keras.Model</a> sub-class.
Use it as a regular TF 2.0 Keras Model and
refer to the TF 2.0 documentation for all matter related to general usage and behavior.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>TF 2.0 models accepts two formats as inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>having all inputs as keyword arguments (like PyTorch models), or</p></li>
<li><p>having all inputs as a list, tuple or dict in the first positional arguments.</p></li>
</ul>
</div></blockquote>
<p>This second option is useful when using <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.Model.fit()</span></code> method which currently requires having
all the tensors in the first argument of the model call function: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs)</span></code>.</p>
<p>If you choose this second option, there are three possibilities you can use to gather all the input Tensors
in the first positional argument :</p>
<ul class="simple">
<li><p>a single Tensor with input_ids only and nothing else: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs_ids)</span></code></p></li>
<li><p>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask])</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">token_type_ids])</span></code></p></li>
<li><p>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model({'input_ids':</span> <span class="pre">input_ids,</span> <span class="pre">'token_type_ids':</span> <span class="pre">token_type_ids})</span></code></p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</p>
</dd>
</dl>
<dl class="method">
<dt id="transformers.TFDistilBertForMaskedLM.call"><a name="//apple_ref/cpp/Method/transformers.TFDistilBertForMaskedLM.call"></a>
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_distilbert.html#TFDistilBertForMaskedLM.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDistilBertForMaskedLM.call" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TFDistilBertForMaskedLM" title="transformers.TFDistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForMaskedLM</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">embedding_dim)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>training</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to activate dropout modules (if set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) during training or to de-activate them
(if set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) for evaluation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl>
<dt>prediction_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>):</dt><dd><p>Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt><dd><p>tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt><dd><p>tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>:</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<code class="xref py py-class docutils literal notranslate"><span class="pre">transformers,DistilBertConfig</span></code>) and inputs</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">TFDistilBertForMaskedLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"Hello, my dog is cute"</span><span class="p">))[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">prediction_scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.TFDistilBertForMaskedLM.get_output_embeddings"><a name="//apple_ref/cpp/Method/transformers.TFDistilBertForMaskedLM.get_output_embeddings"></a>
<code class="sig-name descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_distilbert.html#TFDistilBertForMaskedLM.get_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDistilBertForMaskedLM.get_output_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s output embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping hidden states to vocabulary.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfdistilbertforsequenceclassification">
<h2>TFDistilBertForSequenceClassification<a class="headerlink" href="#tfdistilbertforsequenceclassification" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.TFDistilBertForSequenceClassification"><a name="//apple_ref/cpp/Class/transformers.TFDistilBertForSequenceClassification"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFDistilBertForSequenceClassification</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">*inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_distilbert.html#TFDistilBertForSequenceClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDistilBertForSequenceClassification" title="Permalink to this definition">¶</a></dt>
<dd><p>DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of
the pooled output) e.g. for GLUE tasks.
This model is a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model">tf.keras.Model</a> sub-class.
Use it as a regular TF 2.0 Keras Model and
refer to the TF 2.0 documentation for all matter related to general usage and behavior.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>TF 2.0 models accepts two formats as inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>having all inputs as keyword arguments (like PyTorch models), or</p></li>
<li><p>having all inputs as a list, tuple or dict in the first positional arguments.</p></li>
</ul>
</div></blockquote>
<p>This second option is useful when using <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.Model.fit()</span></code> method which currently requires having
all the tensors in the first argument of the model call function: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs)</span></code>.</p>
<p>If you choose this second option, there are three possibilities you can use to gather all the input Tensors
in the first positional argument :</p>
<ul class="simple">
<li><p>a single Tensor with input_ids only and nothing else: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs_ids)</span></code></p></li>
<li><p>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask])</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">token_type_ids])</span></code></p></li>
<li><p>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model({'input_ids':</span> <span class="pre">input_ids,</span> <span class="pre">'token_type_ids':</span> <span class="pre">token_type_ids})</span></code></p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</p>
</dd>
</dl>
<dl class="method">
<dt id="transformers.TFDistilBertForSequenceClassification.call"><a name="//apple_ref/cpp/Method/transformers.TFDistilBertForSequenceClassification.call"></a>
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_distilbert.html#TFDistilBertForSequenceClassification.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDistilBertForSequenceClassification.call" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TFDistilBertForSequenceClassification" title="transformers.TFDistilBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForSequenceClassification</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">embedding_dim)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>training</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to activate dropout modules (if set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) during training or to de-activate them
(if set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) for evaluation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl>
<dt>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.num_labels)</span></code>):</dt><dd><p>Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt><dd><p>tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt><dd><p>tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>:</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<code class="xref py py-class docutils literal notranslate"><span class="pre">transformers,DistilBertConfig</span></code>) and inputs</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">TFDistilBertForSequenceClassification</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"Hello, my dog is cute"</span><span class="p">))[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfdistilbertforquestionanswering">
<h2>TFDistilBertForQuestionAnswering<a class="headerlink" href="#tfdistilbertforquestionanswering" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.TFDistilBertForQuestionAnswering"><a name="//apple_ref/cpp/Class/transformers.TFDistilBertForQuestionAnswering"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFDistilBertForQuestionAnswering</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">*inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_distilbert.html#TFDistilBertForQuestionAnswering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDistilBertForQuestionAnswering" title="Permalink to this definition">¶</a></dt>
<dd><p>DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of
the hidden-states output to compute <cite>span start logits</cite> and <cite>span end logits</cite>).
This model is a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model">tf.keras.Model</a> sub-class.
Use it as a regular TF 2.0 Keras Model and
refer to the TF 2.0 documentation for all matter related to general usage and behavior.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>TF 2.0 models accepts two formats as inputs:</p>
<blockquote>
<div><ul class="simple">
<li><p>having all inputs as keyword arguments (like PyTorch models), or</p></li>
<li><p>having all inputs as a list, tuple or dict in the first positional arguments.</p></li>
</ul>
</div></blockquote>
<p>This second option is useful when using <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.Model.fit()</span></code> method which currently requires having
all the tensors in the first argument of the model call function: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs)</span></code>.</p>
<p>If you choose this second option, there are three possibilities you can use to gather all the input Tensors
in the first positional argument :</p>
<ul class="simple">
<li><p>a single Tensor with input_ids only and nothing else: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs_ids)</span></code></p></li>
<li><p>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask])</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">token_type_ids])</span></code></p></li>
<li><p>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model({'input_ids':</span> <span class="pre">input_ids,</span> <span class="pre">'token_type_ids':</span> <span class="pre">token_type_ids})</span></code></p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</p>
</dd>
</dl>
<dl class="method">
<dt id="transformers.TFDistilBertForQuestionAnswering.call"><a name="//apple_ref/cpp/Method/transformers.TFDistilBertForQuestionAnswering.call"></a>
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_distilbert.html#TFDistilBertForQuestionAnswering.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDistilBertForQuestionAnswering.call" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TFDistilBertForQuestionAnswering" title="transformers.TFDistilBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForQuestionAnswering</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">embedding_dim)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>training</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to activate dropout modules (if set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) during training or to de-activate them
(if set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) for evaluation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl>
<dt>start_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,)</span></code>):</dt><dd><p>Span-start scores (before SoftMax).</p>
</dd>
<dt>end_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,)</span></code>):</dt><dd><p>Span-end scores (before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt><dd><p>tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt><dd><p>tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>:</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<code class="xref py py-class docutils literal notranslate"><span class="pre">transformers,DistilBertConfig</span></code>) and inputs</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">TFDistilBertForQuestionAnswering</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilbert-base-cased'</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"Hello, my dog is cute"</span><span class="p">))[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">start_scores</span><span class="p">,</span> <span class="n">end_scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="ctrl.html" rel="next" title="CTRL">Next <span class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="roberta.html" rel="prev" title="RoBERTa"><span class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        © Copyright 2020, huggingface

    </p>
</div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    ga('send', 'pageview');
    </script>
</body>
</html>