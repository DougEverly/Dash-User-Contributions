

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Quickstart &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/js/custom.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/hidesidebar.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Glossary" href="glossary.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#philosophy">Philosophy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#main-concepts">Main concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quick-tour-usage">Quick tour: Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#bert-example">BERT example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openai-gpt-2">OpenAI GPT-2</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-the-past">Using the past</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_sharing.html">Model upload and sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchscript.html">TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_doc/auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Quickstart</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/quickstart.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="quickstart">
<h1>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this headline">¶</a></h1>
<div class="section" id="philosophy">
<h2>Philosophy<a class="headerlink" href="#philosophy" title="Permalink to this headline">¶</a></h2>
<p>Transformers is an opinionated library built for NLP researchers seeking to use/study/extend large-scale transformers models.</p>
<p>The library was designed with two strong goals in mind:</p>
<ul class="simple">
<li><p>be as easy and fast to use as possible:</p>
<ul>
<li><p>we strongly limited the number of user-facing abstractions to learn, in fact there are almost no abstractions, just three standard classes required to use each model: configuration, models and tokenizer,</p></li>
<li><p>all of these classes can be initialized in a simple and unified way from pretrained instances by using a common <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> instantiation method which will take care of downloading (if needed), caching and loading the related class from a pretrained instance supplied in the library or your own saved instance.</p></li>
<li><p>as a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to extend/build-upon the library, just use regular Python/PyTorch modules and inherit from the base classes of the library to reuse functionalities like model loading/saving.</p></li>
</ul>
</li>
<li><p>provide state-of-the-art models with performances as close as possible to the original models:</p>
<ul>
<li><p>we provide at least one example for each architecture which reproduces a result provided by the official authors of said architecture,</p></li>
<li><p>the code is usually as close to the original code base as possible which means some PyTorch code may be not as <em>pytorchic</em> as it could be as a result of being converted TensorFlow code.</p></li>
</ul>
</li>
</ul>
<p>A few other goals:</p>
<ul class="simple">
<li><p>expose the models’ internals as consistently as possible:</p>
<ul>
<li><p>we give access, using a single API to the full hidden-states and attention weights,</p></li>
<li><p>tokenizer and base model’s API are standardized to easily switch between models.</p></li>
</ul>
</li>
<li><p>incorporate a subjective selection of promising tools for fine-tuning/investigating these models:</p>
<ul>
<li><p>a simple/consistent way to add new tokens to the vocabulary and embeddings for fine-tuning,</p></li>
<li><p>simple ways to mask and prune transformer heads.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="main-concepts">
<h2>Main concepts<a class="headerlink" href="#main-concepts" title="Permalink to this headline">¶</a></h2>
<p>The library is build around three type of classes for each models:</p>
<ul class="simple">
<li><p><strong>model classes</strong> which are PyTorch models (<code class="docutils literal notranslate"><span class="pre">torch.nn.Modules</span></code>) of the 8 models architectures currently provided in the library, e.g. <code class="docutils literal notranslate"><span class="pre">BertModel</span></code></p></li>
<li><p><strong>configuration classes</strong> which store all the parameters required to build a model, e.g. <code class="docutils literal notranslate"><span class="pre">BertConfig</span></code>. You don’t always need to instantiate these your-self, in particular if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model)</p></li>
<li><p><strong>tokenizer classes</strong> which store the vocabulary for each model and provide methods for encoding/decoding strings in list of token embeddings indices to be fed to a model, e.g. <code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code></p></li>
</ul>
<p>All these classes can be instantiated from pretrained instances and saved locally using two methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> let you instantiate a model/configuration/tokenizer from a pretrained version either provided by the library itself (currently 27 models are provided as listed <a class="reference external" href="https://huggingface.co/transformers/pretrained_models.html">here</a>) or stored locally (or on a server) by the user,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">save_pretrained()</span></code> let you save a model/configuration/tokenizer locally so that it can be reloaded using <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code>.</p></li>
</ul>
<p>We’ll finish this quickstart tour by going through a few simple quick-start examples to see how we can instantiate and use these classes. The rest of the documentation is organized in two parts:</p>
<ul class="simple">
<li><p>the <strong>MAIN CLASSES</strong> section details the common functionalities/method/attributes of the three main type of classes (configuration, model, tokenizer) plus some optimization related classes provided as utilities for training,</p></li>
<li><p>the <strong>PACKAGE REFERENCE</strong> section details all the variants of each class for each model architectures and in particular the input/output that you should expect when calling each of them.</p></li>
</ul>
</div>
<div class="section" id="quick-tour-usage">
<h2>Quick tour: Usage<a class="headerlink" href="#quick-tour-usage" title="Permalink to this headline">¶</a></h2>
<p>Here are two examples showcasing a few <code class="docutils literal notranslate"><span class="pre">Bert</span></code> and <code class="docutils literal notranslate"><span class="pre">GPT2</span></code> classes and pre-trained models.</p>
<p>See full API reference for examples for each model class.</p>
<div class="section" id="bert-example">
<h3>BERT example<a class="headerlink" href="#bert-example" title="Permalink to this headline">¶</a></h3>
<p>Let’s start by preparing a tokenized input (a list of token embeddings indices to be fed to Bert) from a text string using <code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertForMaskedLM</span>

<span class="c1"># OPTIONAL: if you want to have more information on what&#39;s happening under the hood, activate the logger as follows</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="c1"># Load pre-trained model tokenizer (vocabulary)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># Tokenize input</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;</span>
<span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Mask a token that we will try to predict back with `BertForMaskedLM`</span>
<span class="n">masked_index</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">tokenized_text</span><span class="p">[</span><span class="n">masked_index</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;[MASK]&#39;</span>
<span class="k">assert</span> <span class="n">tokenized_text</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">,</span> <span class="s1">&#39;who&#39;</span><span class="p">,</span> <span class="s1">&#39;was&#39;</span><span class="p">,</span> <span class="s1">&#39;jim&#39;</span><span class="p">,</span> <span class="s1">&#39;henson&#39;</span><span class="p">,</span> <span class="s1">&#39;?&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">,</span> <span class="s1">&#39;jim&#39;</span><span class="p">,</span> <span class="s1">&#39;[MASK]&#39;</span><span class="p">,</span> <span class="s1">&#39;was&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;puppet&#39;</span><span class="p">,</span> <span class="s1">&#39;##eer&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">]</span>

<span class="c1"># Convert token to vocabulary indices</span>
<span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
<span class="c1"># Define sentence A and B indices associated to 1st and 2nd sentences (see paper)</span>
<span class="n">segments_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Convert inputs to PyTorch tensors</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">segments_ids</span><span class="p">])</span>
</pre></div>
</div>
<p>Let’s see how we can use <code class="docutils literal notranslate"><span class="pre">BertModel</span></code> to encode our inputs in hidden-states:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load pre-trained model (weights)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># Set the model in evaluation mode to deactivate the DropOut modules</span>
<span class="c1"># This is IMPORTANT to have reproducible results during evaluation!</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># If you have a GPU, put everything on cuda</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">tokens_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">segments_tensors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="c1"># Predict hidden states features for each layer</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># See the models docstrings for the detail of the inputs</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">)</span>
    <span class="c1"># Transformers models always output tuples.</span>
    <span class="c1"># See the models docstrings for the detail of all the outputs</span>
    <span class="c1"># In our case, the first element is the hidden state of the last layer of the Bert model</span>
    <span class="n">encoded_layers</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)</span>
<span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">encoded_layers</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indexed_tokens</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
</pre></div>
</div>
<p>And how to use <code class="docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code> to predict a masked token:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load pre-trained model (weights)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># If you have a GPU, put everything on cuda</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">tokens_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">segments_tensors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="c1"># Predict all tokens</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># confirm we were able to predict &#39;henson&#39;</span>
<span class="n">predicted_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">masked_index</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">predicted_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">([</span><span class="n">predicted_index</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">predicted_token</span> <span class="o">==</span> <span class="s1">&#39;henson&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="openai-gpt-2">
<h3>OpenAI GPT-2<a class="headerlink" href="#openai-gpt-2" title="Permalink to this headline">¶</a></h3>
<p>Here is a quick-start example using <code class="docutils literal notranslate"><span class="pre">GPT2Tokenizer</span></code> and <code class="docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code> class with OpenAI’s pre-trained model to predict the next token from a text prompt.</p>
<p>First let’s prepare a tokenized input from our text string using <code class="docutils literal notranslate"><span class="pre">GPT2Tokenizer</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>

<span class="c1"># OPTIONAL: if you want to have more information on what&#39;s happening, activate the logger as follows</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="c1"># Load pre-trained model tokenizer (vocabulary)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="c1"># Encode a text inputs</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Who was Jim Henson ? Jim Henson was a&quot;</span>
<span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Convert indexed tokens in a PyTorch tensor</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>
</pre></div>
</div>
<p>Let’s see how to use <code class="docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code> to generate the next token following our text:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load pre-trained model (weights)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="c1"># Set the model in evaluation mode to deactivate the DropOut modules</span>
<span class="c1"># This is IMPORTANT to have reproducible results during evaluation!</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># If you have a GPU, put everything on cuda</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">tokens_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="c1"># Predict all tokens</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># get the predicted next sub-word (in our case, the word &#39;man&#39;)</span>
<span class="n">predicted_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">predicted_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">indexed_tokens</span> <span class="o">+</span> <span class="p">[</span><span class="n">predicted_index</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">predicted_text</span> <span class="o">==</span> <span class="s1">&#39;Who was Jim Henson? Jim Henson was a man&#39;</span>
</pre></div>
</div>
<p>Examples for each model class of each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the <a class="reference external" href="#documentation">documentation</a>.</p>
<div class="section" id="using-the-past">
<h4>Using the past<a class="headerlink" href="#using-the-past" title="Permalink to this headline">¶</a></h4>
<p>GPT-2 as well as some other models (GPT, XLNet, Transfo-XL, CTRL) make use of a <code class="docutils literal notranslate"><span class="pre">past</span></code> or <code class="docutils literal notranslate"><span class="pre">mems</span></code> attribute which can be used to prevent re-computing the key/value pairs when using sequential decoding. It is useful when generating sequences as a big part of the attention mechanism benefits from previous computations.</p>
<p>Here is a fully-working example using the <code class="docutils literal notranslate"><span class="pre">past</span></code> with <code class="docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code> and argmax decoding (which should only be used as an example, as argmax decoding introduces a lot of repetition):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="n">generated</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;The Manhattan bridge&quot;</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">generated</span><span class="p">])</span>
<span class="n">past</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">past</span><span class="o">=</span><span class="n">past</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

    <span class="n">generated</span> <span class="o">+=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">sequence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">generated</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
</pre></div>
</div>
<p>The model only requires a single token as input as all the previous tokens’ key/value pairs are contained in the <code class="docutils literal notranslate"><span class="pre">past</span></code>.</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="glossary.html" class="btn btn-neutral float-right" title="Glossary" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>