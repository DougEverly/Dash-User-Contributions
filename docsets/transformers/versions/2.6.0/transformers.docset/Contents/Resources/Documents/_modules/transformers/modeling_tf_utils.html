

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>transformers.modeling_tf_utils &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/custom.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/hidesidebar.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_sharing.html">Model upload and sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torchscript.html">TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>transformers.modeling_tf_utils</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for transformers.modeling_tf_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.</span>
<span class="c1"># Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;TF general model utils.&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">h5py</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.saving</span> <span class="kn">import</span> <span class="n">hdf5_format</span>

<span class="kn">from</span> <span class="nn">.configuration_utils</span> <span class="kn">import</span> <span class="n">PretrainedConfig</span>
<span class="kn">from</span> <span class="nn">.file_utils</span> <span class="kn">import</span> <span class="n">DUMMY_INPUTS</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">cached_path</span><span class="p">,</span> <span class="n">hf_bucket_url</span><span class="p">,</span> <span class="n">is_remote_url</span>
<span class="kn">from</span> <span class="nn">.modeling_tf_pytorch_utils</span> <span class="kn">import</span> <span class="n">load_pytorch_checkpoint_in_tf2_model</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TFModelUtilsMixin</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A few utilities for `tf.keras.Model`s, to be used as a mixin.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">num_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">only_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get number of (optionally, trainable) parameters in the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">only_trainable</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_params</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">keras_serializable</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Decorate a Keras Layer class to support Keras serialization.</span>

<span class="sd">    This is done by:</span>
<span class="sd">    1. adding a `transformers_config` dict to the Keras config dictionary in `get_config` (called by Keras at</span>
<span class="sd">       serialization time</span>
<span class="sd">    2. wrapping `__init__` to accept that `transformers_config` dict (passed by Keras at deserialization time) and</span>
<span class="sd">       convert it to a config object for the actual layer initializer</span>
<span class="sd">    3. registering the class as a custom object in Keras (if the Tensorflow version supports this), so that it does</span>
<span class="sd">       not need to be supplied in `custom_objects` in the call to `tf.keras.models.load_model`</span>

<span class="sd">    :param cls: a tf.keras.layers.Layers subclass that accepts a `config` argument to its initializer (typically a</span>
<span class="sd">                `TF*MainLayer` class in this project)</span>
<span class="sd">    :return: the same class object, with modifications for Keras deserialization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">initializer</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="fm">__init__</span>

    <span class="n">config_class</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;config_class&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;Must set `config_class` to use @keras_serializable&quot;</span><span class="p">)</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">transformers_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;transformers_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">args</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">PretrainedConfig</span><span class="p">)</span> <span class="k">else</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">transformers_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must pass either `config` or `transformers_config`, not both&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># normal layer construction, call with unchanged args (config is already in there)</span>
            <span class="n">initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">transformers_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Keras deserialization, convert dict to config</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">config_class</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">transformers_config</span><span class="p">)</span>
            <span class="n">initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must pass either `config` (PretrainedConfig) or `transformers_config` (dict)&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_transformers_config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="bp">cls</span><span class="o">.</span><span class="fm">__init__</span> <span class="o">=</span> <span class="n">wrapped_init</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;get_config&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Only use @keras_serializable on tf.keras.layers.Layer subclasses&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">get_config</span><span class="p">,</span> <span class="s2">&quot;_is_default&quot;</span><span class="p">):</span>

        <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">cfg</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
            <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;transformers_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transformers_config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">cfg</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">get_config</span> <span class="o">=</span> <span class="n">get_config</span>

    <span class="bp">cls</span><span class="o">.</span><span class="n">_keras_serializable</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="p">,</span> <span class="s2">&quot;register_keras_serializable&quot;</span><span class="p">):</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">register_keras_serializable</span><span class="p">()(</span><span class="bp">cls</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">cls</span>


<div class="viewcode-block" id="TFPreTrainedModel"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFPreTrainedModel">[docs]</a><span class="k">class</span> <span class="nc">TFPreTrainedModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">,</span> <span class="n">TFModelUtilsMixin</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Base class for all TF models.</span>

<span class="sd">        :class:`~transformers.TFPreTrainedModel` takes care of storing the configuration of the models and handles methods for loading/downloading/saving models</span>
<span class="sd">        as well as a few methods common to all models to (i) resize the input embeddings and (ii) prune heads in the self-attention heads.</span>

<span class="sd">        Class attributes (overridden by derived classes):</span>
<span class="sd">            - ``config_class``: a class derived from :class:`~transformers.PretrainedConfig` to use as configuration class for this model architecture.</span>
<span class="sd">            - ``pretrained_model_archive_map``: a python ``dict`` of with `short-cut-names` (string) as keys and `url` (string) of associated pretrained weights as values.</span>
<span class="sd">            - ``load_tf_weights``: a python ``method`` for loading a TensorFlow checkpoint in a PyTorch model, taking as arguments:</span>

<span class="sd">                - ``model``: an instance of the relevant subclass of :class:`~transformers.PreTrainedModel`,</span>
<span class="sd">                - ``config``: an instance of the relevant subclass of :class:`~transformers.PretrainedConfig`,</span>
<span class="sd">                - ``path``: a path (string) to the TensorFlow checkpoint.</span>

<span class="sd">            - ``base_model_prefix``: a string indicating the attribute associated to the base model in derived classes of the same architecture adding modules on top of the base model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">pretrained_model_archive_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dummy_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Dummy inputs to build the network.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tf.Tensor with dummy inputs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">DUMMY_INPUTS</span><span class="p">)}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Parameter config in `</span><span class="si">{}</span><span class="s2">(config)` should be an instance of class `PretrainedConfig`. &quot;</span>
                <span class="s2">&quot;To create a model from a pretrained model use &quot;</span>
                <span class="s2">&quot;`model = </span><span class="si">{}</span><span class="s2">.from_pretrained(PRETRAINED_MODEL_NAME)`&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># Save config in model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

<div class="viewcode-block" id="TFPreTrainedModel.get_input_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFPreTrainedModel.get_input_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the model&#39;s input embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`tf.keras.layers.Layer`:</span>
<span class="sd">                A torch module mapping vocabulary to hidden states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">base_model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="TFPreTrainedModel.get_output_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFPreTrainedModel.get_output_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the model&#39;s output embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`tf.keras.layers.Layer`:</span>
<span class="sd">                A torch module mapping hidden states to vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Overwrite for models with output embeddings</span></div>

    <span class="k">def</span> <span class="nf">_get_resized_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">old_embeddings</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Build a resized Embedding Variable from a provided token Embedding Module.</span>
<span class="sd">            Increasing the size will add newly initialized vectors at the end</span>
<span class="sd">            Reducing the size will remove vectors from the end</span>

<span class="sd">        Args:</span>
<span class="sd">            new_num_tokens: (`optional`) int</span>
<span class="sd">                New number of tokens in the embedding matrix.</span>
<span class="sd">                Increasing the size will add newly initialized vectors at the end</span>
<span class="sd">                Reducing the size will remove vectors from the end</span>
<span class="sd">                If not provided or None: return the provided token Embedding Module.</span>
<span class="sd">        Return: ``tf.Variable``</span>
<span class="sd">            Pointer to the resized Embedding Module or the old Embedding Module if new_num_tokens is None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if new_num_tokens is None:</span>
        <span class="c1">#     return old_embeddings</span>

        <span class="c1"># old_num_tokens, old_embedding_dim = old_embeddings.weight.size()</span>
        <span class="c1"># if old_num_tokens == new_num_tokens:</span>
        <span class="c1">#     return old_embeddings</span>

        <span class="c1"># # Build new embeddings</span>
        <span class="c1"># new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)</span>
        <span class="c1"># new_embeddings.to(old_embeddings.weight.device)</span>

        <span class="c1"># # initialize all new embeddings (in particular added tokens)</span>
        <span class="c1"># self._init_weights(new_embeddings)</span>

        <span class="c1"># # Copy token embeddings from the previous weights</span>
        <span class="c1"># num_tokens_to_copy = min(old_num_tokens, new_num_tokens)</span>
        <span class="c1"># new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]</span>

        <span class="c1"># return new_embeddings</span>

<div class="viewcode-block" id="TFPreTrainedModel.resize_token_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFPreTrainedModel.resize_token_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">resize_token_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.</span>
<span class="sd">        Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.</span>

<span class="sd">        Arguments:</span>

<span class="sd">            new_num_tokens: (`optional`) int:</span>
<span class="sd">                New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end.</span>
<span class="sd">                If not provided or None: does nothing and just returns a pointer to the input tokens ``tf.Variable`` Module of the model.</span>

<span class="sd">        Return: ``tf.Variable``</span>
<span class="sd">            Pointer to the input tokens Embeddings Module of the model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="TFPreTrainedModel.prune_heads"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFPreTrainedModel.prune_heads">[docs]</a>    <span class="k">def</span> <span class="nf">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads_to_prune</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Prunes heads of the base model.</span>

<span class="sd">            Arguments:</span>

<span class="sd">                heads_to_prune: dict with keys being selected layer indices (`int`) and associated values being the list of heads to prune in said layer (list of `int`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="TFPreTrainedModel.save_pretrained"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFPreTrainedModel.save_pretrained">[docs]</a>    <span class="k">def</span> <span class="nf">save_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Save a model and its configuration file to a directory, so that it</span>
<span class="sd">            can be re-loaded using the :func:`~transformers.PreTrainedModel.from_pretrained` class method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span>
            <span class="n">save_directory</span>
        <span class="p">),</span> <span class="s2">&quot;Saving path should be a directory where the model and configuration can be saved&quot;</span>

        <span class="c1"># Save configuration file</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

        <span class="c1"># If we save using the predefined names, we can load using `from_pretrained`</span>
        <span class="n">output_model_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="n">output_model_file</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Model weights saved in </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_model_file</span><span class="p">))</span></div>

<div class="viewcode-block" id="TFPreTrainedModel.from_pretrained"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFPreTrainedModel.from_pretrained">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">model_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Instantiate a pretrained TF 2.0 model from a pre-trained model configuration.</span>

<span class="sd">        The warning ``Weights from XXX not initialized from pretrained model`` means that the weights of XXX do not come pre-trained with the rest of the model.</span>
<span class="sd">        It is up to you to train those weights with a downstream fine-tuning task.</span>

<span class="sd">        The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used by YYY, therefore those weights are discarded.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path: either:</span>

<span class="sd">                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.</span>
<span class="sd">                - a string with the `identifier name` of a pre-trained model that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.</span>
<span class="sd">                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.</span>
<span class="sd">                - a path or url to a `PyTorch state_dict save file` (e.g. `./pt_model/pytorch_model.bin`). In this case, ``from_pt`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the PyTorch checkpoint in a TensorFlow model using the provided conversion scripts and loading the TensorFlow model afterwards.</span>

<span class="sd">            model_args: (`optional`) Sequence of positional arguments:</span>
<span class="sd">                All remaning positional arguments will be passed to the underlying model&#39;s ``__init__`` method</span>

<span class="sd">            config: (`optional`) one of:</span>
<span class="sd">                    - an instance of a class derived from :class:`~transformers.PretrainedConfig`, or</span>
<span class="sd">                    - a string valid as input to :func:`~transformers.PretrainedConfig.from_pretrained()`</span>
<span class="sd">                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</span>

<span class="sd">                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or</span>
<span class="sd">                - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.</span>
<span class="sd">                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.</span>

<span class="sd">            from_pt: (`optional`) boolean, default False:</span>
<span class="sd">                Load the model weights from a PyTorch state_dict save file (see docstring of pretrained_model_name_or_path argument).</span>

<span class="sd">            cache_dir: (`optional`) string:</span>
<span class="sd">                Path to a directory in which a downloaded pre-trained model</span>
<span class="sd">                configuration should be cached if the standard cache should not be used.</span>

<span class="sd">            force_download: (`optional`) boolean, default False:</span>
<span class="sd">                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</span>

<span class="sd">            resume_download: (`optional`) boolean, default False:</span>
<span class="sd">                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</span>

<span class="sd">            proxies: (`optional`) dict, default None:</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {&#39;http&#39;: &#39;foo.bar:3128&#39;, &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}.</span>
<span class="sd">                The proxies are used on each request.</span>

<span class="sd">            output_loading_info: (`optional`) boolean:</span>
<span class="sd">                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.</span>

<span class="sd">            kwargs: (`optional`) Remaining dictionary of keyword arguments:</span>
<span class="sd">                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:</span>

<span class="sd">                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model&#39;s ``__init__`` method (we assume all relevant updates to the configuration have already been done)</span>
<span class="sd">                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model&#39;s ``__init__`` function.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # For example purposes. Not runnable.</span>
<span class="sd">            model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;)    # Download model and configuration from S3 and cache.</span>
<span class="sd">            model = BertModel.from_pretrained(&#39;./test/saved_model/&#39;)  # E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
<span class="sd">            model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;, output_attention=True)  # Update configuration during loading</span>
<span class="sd">            assert model.config.output_attention == True</span>
<span class="sd">            # Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="sd">            config = BertConfig.from_json_file(&#39;./tf_model/my_tf_model_config.json&#39;)</span>
<span class="sd">            model = BertModel.from_pretrained(&#39;./tf_model/my_tf_checkpoint.ckpt.index&#39;, from_pt=True, config=config)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">from_pt</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;from_pt&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">resume_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;resume_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">output_loading_info</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_loading_info&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Load config if we don&#39;t provide a configuration</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
            <span class="n">config_path</span> <span class="o">=</span> <span class="n">config</span> <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="n">config</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">config_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="n">config_path</span><span class="p">,</span>
                <span class="o">*</span><span class="n">model_args</span><span class="p">,</span>
                <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="c1"># Load model</span>
        <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_model_archive_map</span><span class="p">:</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_model_archive_map</span><span class="p">[</span><span class="n">pretrained_model_name_or_path</span><span class="p">]</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">)):</span>
                    <span class="c1"># Load from a TF 2.0 checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">from_pt</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)):</span>
                    <span class="c1"># Load from a PyTorch checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                        <span class="s2">&quot;Error no file named </span><span class="si">{}</span><span class="s2"> found in directory </span><span class="si">{}</span><span class="s2"> or `from_pt` set to False&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="p">[</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">],</span> <span class="n">pretrained_model_name_or_path</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_remote_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span><span class="p">):</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">hf_bucket_url</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">postfix</span><span class="o">=</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span> <span class="k">if</span> <span class="n">from_pt</span> <span class="k">else</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># redirect to the cache, if necessary</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_path</span><span class="p">(</span>
                    <span class="n">archive_file</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                    <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                    <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">EnvironmentError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_model_archive_map</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Couldn&#39;t reach server at &#39;</span><span class="si">{}</span><span class="s2">&#39; to download pretrained weights.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">archive_file</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                        <span class="s2">&quot;Model name &#39;</span><span class="si">{}</span><span class="s2">&#39; was not found in model name list (</span><span class="si">{}</span><span class="s2">). &quot;</span>
                        <span class="s2">&quot;We assumed &#39;</span><span class="si">{}</span><span class="s2">&#39; was a path or url but couldn&#39;t find any file &quot;</span>
                        <span class="s2">&quot;associated to this path or url.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                            <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_model_archive_map</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                            <span class="n">archive_file</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">raise</span> <span class="n">e</span>
            <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="o">==</span> <span class="n">archive_file</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading weights file </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">archive_file</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading weights file </span><span class="si">{}</span><span class="s2"> from cache at </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">archive_file</span><span class="p">,</span> <span class="n">resolved_archive_file</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Instantiate model.</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">model_args</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">from_pt</span><span class="p">:</span>
            <span class="c1"># Load from a PyTorch checkpoint</span>
            <span class="k">return</span> <span class="n">load_pytorch_checkpoint_in_tf2_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">allow_missing_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">model</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dummy_inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># build the network with dummy inputs</span>

        <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">),</span> <span class="s2">&quot;Error retrieving file </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">)</span>
        <span class="c1"># &#39;by_name&#39; allow us to do transfer learning by skipping/adding layers</span>
        <span class="c1"># see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">by_name</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">OSError</span><span class="p">(</span>
                <span class="s2">&quot;Unable to load weights from h5 file. &quot;</span>
                <span class="s2">&quot;If you tried to load a TF 2.0 model from a PyTorch checkpoint, please set from_pt=True. &quot;</span>
            <span class="p">)</span>

        <span class="n">model</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dummy_inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Make sure restore ops are run</span>

        <span class="c1"># Check if the models are the same to output loading informations</span>
        <span class="k">with</span> <span class="n">h5py</span><span class="o">.</span><span class="n">File</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;layer_names&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">attrs</span> <span class="ow">and</span> <span class="s2">&quot;model_weights&quot;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">f</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="s2">&quot;model_weights&quot;</span><span class="p">]</span>
            <span class="n">hdf5_layer_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">hdf5_format</span><span class="o">.</span><span class="n">load_attributes_from_hdf5_group</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s2">&quot;layer_names&quot;</span><span class="p">))</span>
        <span class="n">model_layer_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
        <span class="n">missing_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model_layer_names</span> <span class="o">-</span> <span class="n">hdf5_layer_names</span><span class="p">)</span>
        <span class="n">unexpected_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">hdf5_layer_names</span> <span class="o">-</span> <span class="n">model_layer_names</span><span class="p">)</span>
        <span class="n">error_msgs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Layers of </span><span class="si">{}</span><span class="s2"> not initialized from pretrained model: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Layers from pretrained model not used in </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Error(s) in loading weights for </span><span class="si">{}</span><span class="s2">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">output_loading_info</span><span class="p">:</span>
            <span class="n">loading_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;missing_keys&quot;</span><span class="p">:</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="s2">&quot;unexpected_keys&quot;</span><span class="p">:</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="s2">&quot;error_msgs&quot;</span><span class="p">:</span> <span class="n">error_msgs</span><span class="p">}</span>
            <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">loading_info</span>

        <span class="k">return</span> <span class="n">model</span></div>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">_do_output_past</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="n">has_output_past</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;output_past&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_past</span>
        <span class="n">has_mem_len</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;mem_len&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mem_len</span>

        <span class="k">if</span> <span class="n">has_output_past</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">has_mem_len</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">has_mem_len</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mem_len</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="kc">False</span>

<div class="viewcode-block" id="TFPreTrainedModel.generate"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFPreTrainedModel.generate">[docs]</a>    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">early_stopping</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">length_penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_return_sequences</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Generates sequences for models with a LM head. The method currently supports greedy or penalized greedy decoding, sampling with top-k or nucleus sampling</span>
<span class="sd">        and beam-search.</span>

<span class="sd">        Adapted in part from `Facebook&#39;s XLM beam search code`_.</span>

<span class="sd">        .. _`Facebook&#39;s XLM beam search code`:</span>
<span class="sd">           https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529</span>


<span class="sd">        Parameters:</span>

<span class="sd">            input_ids: (`optional`) `tf.Tensor` of `dtype=tf.int32` of shape `(batch_size, sequence_length)`</span>
<span class="sd">                The sequence used as a prompt for the generation. If `None` the method initializes</span>
<span class="sd">                it as an empty `torch.LongTensor` of shape `(1,)`.</span>

<span class="sd">            max_length: (`optional`) int</span>
<span class="sd">                The max length of the sequence to be generated.  Between 1 and infinity. Default to 20.</span>

<span class="sd">            min_length: (`optional`) int</span>
<span class="sd">                The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.</span>
<span class="sd">            do_sample: (`optional`) bool</span>
<span class="sd">                If set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.</span>

<span class="sd">            early_stopping: (`optional`) bool</span>
<span class="sd">                if set to `True` beam search is stopped when at least `num_beams` sentences finished per batch. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.</span>

<span class="sd">            num_beams: (`optional`) int</span>
<span class="sd">                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.</span>

<span class="sd">            temperature: (`optional`) float</span>
<span class="sd">                The value used to module the next token probabilities. Must be strictely positive. Default to 1.0.</span>

<span class="sd">            top_k: (`optional`) int</span>
<span class="sd">                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.</span>

<span class="sd">            top_p: (`optional`) float</span>
<span class="sd">                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.</span>

<span class="sd">            repetition_penalty: (`optional`) float</span>
<span class="sd">                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.</span>

<span class="sd">            bos_token_id: (`optional`) int</span>
<span class="sd">                Beginning of sentence token if no prompt is provided. Default to specicic model bos_token_id or None if it does not exist.</span>

<span class="sd">            pad_token_id: (`optional`) int</span>
<span class="sd">                Pad token. Defaults to pad_token_id as defined in the models config.</span>

<span class="sd">            eos_token_ids: (`optional`) int or list of int</span>
<span class="sd">                End of sequence token or list of tokens to stop the generation. Default to 0.</span>

<span class="sd">            length_penalty: (`optional`) float</span>
<span class="sd">                Exponential penalty to the length. Default to 1.</span>

<span class="sd">            no_repeat_ngram_size: (`optional`) int</span>
<span class="sd">                If set to int &gt; 0, all ngrams of size `no_repeat_ngram_size` can only occur once.</span>

<span class="sd">            num_return_sequences: (`optional`) int</span>
<span class="sd">                The number of independently computed returned sequences for each element in the batch. Default to 1.</span>

<span class="sd">            attention_mask (`optional`) obj: `tf.Tensor` with `dtype=tf.int32` of same shape as `input_ids`</span>
<span class="sd">                Mask to avoid performing attention on padding token indices.</span>
<span class="sd">                Mask values selected in ``[0, 1]``:</span>
<span class="sd">                ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.</span>
<span class="sd">                Defaults to `None`.</span>

<span class="sd">                `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>

<span class="sd">            decoder_start_token_id=None: (`optional`) int</span>
<span class="sd">                If an encoder-decoder model starts decoding with a different token than BOS.</span>
<span class="sd">                Defaults to `None` and is changed to `BOS` later.</span>

<span class="sd">        Return:</span>

<span class="sd">            output: `tf.Tensor` of `dtype=tf.int32` shape `(batch_size * num_return_sequences, sequence_length)`</span>
<span class="sd">                sequence_length is either equal to max_length or shorter if all batches finished early due to the `eos_token_id`</span>

<span class="sd">        Examples::</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;distilgpt2&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = TFAutoModelWithLMHead.from_pretrained(&#39;distilgpt2&#39;)    # Download model and configuration from S3 and cache.</span>
<span class="sd">            outputs = model.generate(max_length=40)  # do greedy decoding</span>
<span class="sd">            print(&#39;Generated: {}&#39;.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;openai-gpt&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = TFAutoModelWithLMHead.from_pretrained(&#39;openai-gpt&#39;)    # Download model and configuration from S3 and cache.</span>
<span class="sd">            input_context = &#39;The dog&#39;</span>
<span class="sd">            input_ids = tokenizer.encode(input_context, return_tensors=&#39;tf&#39;)  # encode input context</span>
<span class="sd">            outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context &#39;The dog&#39;</span>
<span class="sd">            for i in range(3): #  3 output sequences were generated</span>
<span class="sd">                print(&#39;Generated {}: {}&#39;.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;distilgpt2&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = TFAutoModelWithLMHead.from_pretrained(&#39;distilgpt2&#39;)    # Download model and configuration from S3 and cache.</span>
<span class="sd">            input_context = &#39;The dog&#39;</span>
<span class="sd">            input_ids = tokenizer.encode(input_context, return_tensors=&#39;tf&#39;)  # encode input context</span>
<span class="sd">            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3)  # 3 generate sequences using by sampling</span>
<span class="sd">            for i in range(3): #  3 output sequences were generated</span>
<span class="sd">                print(&#39;Generated {}: {}&#39;.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;ctrl&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = TFAutoModelWithLMHead.from_pretrained(&#39;ctrl&#39;)    # Download model and configuration from S3 and cache.</span>
<span class="sd">            input_context = &#39;Legal My neighbor is&#39;  # &quot;Legal&quot; is one of the control codes for ctrl</span>
<span class="sd">            input_ids = tokenizer.encode(input_context, return_tensors=&#39;tf&#39;)  # encode input context</span>
<span class="sd">            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences</span>
<span class="sd">            print(&#39;Generated: {}&#39;.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># We cannot generate if the model does not have a LM head</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;You tried to generate sequences with a model that does not have a LM Head.&quot;</span>
                <span class="s2">&quot;Please use another model class (e.g. `TFOpenAIGPTLMHeadModel`, `TFXLNetLMHeadModel`, `TFGPT2LMHeadModel`, `TFCTRLLMHeadModel`, `TFT5ForConditionalGeneration`, `TFTransfoXLLMHeadModel`)&quot;</span>
            <span class="p">)</span>

        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="n">min_length</span> <span class="o">=</span> <span class="n">min_length</span> <span class="k">if</span> <span class="n">min_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_length</span>
        <span class="n">do_sample</span> <span class="o">=</span> <span class="n">do_sample</span> <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">do_sample</span>
        <span class="n">early_stopping</span> <span class="o">=</span> <span class="n">early_stopping</span> <span class="k">if</span> <span class="n">early_stopping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">early_stopping</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">num_beams</span> <span class="k">if</span> <span class="n">num_beams</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_beams</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span> <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">temperature</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="n">top_k</span> <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">top_k</span>
        <span class="n">top_p</span> <span class="o">=</span> <span class="n">top_p</span> <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">top_p</span>
        <span class="n">repetition_penalty</span> <span class="o">=</span> <span class="n">repetition_penalty</span> <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">repetition_penalty</span>
        <span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span> <span class="k">if</span> <span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span> <span class="k">if</span> <span class="n">length_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">length_penalty</span>
        <span class="n">no_repeat_ngram_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">no_repeat_ngram_size</span> <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">no_repeat_ngram_size</span>
        <span class="p">)</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">num_return_sequences</span> <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_return_sequences</span>
        <span class="p">)</span>
        <span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decoder_start_token_id</span> <span class="k">if</span> <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># overriden by the input batch_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`max_length` should be a strictely positive integer.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">min_length</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`min_length` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">do_sample</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;`do_sample` should be a boolean.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">early_stopping</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;`early_stopping` should be a boolean.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`num_beams` should be a strictely positive integer.&quot;</span>
        <span class="k">assert</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`temperature` should be strictely positive.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">top_k</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`top_k` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">top_p</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;`top_p` should be between 0 and 1.&quot;</span>
        <span class="k">assert</span> <span class="n">repetition_penalty</span> <span class="o">&gt;=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;`repetition_penalty` should be &gt;= 1.&quot;</span>
        <span class="k">assert</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">bos_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;If input_ids is not defined, `bos_token_id` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;`pad_token_id` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;`eos_token_id` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="n">length_penalty</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`length_penalty` should be strictely positive.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_return_sequences</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;`num_return_sequences` should be a strictely positive integer.&quot;</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">bos_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;you should either supply a context to complete as `input_ids` input &quot;</span>
                <span class="s2">&quot;or a `bos_token_id` (integer &gt;= 0) as a first token to start the generation.&quot;</span>
            <span class="p">)</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bos_token_id</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Input prompt should be of shape (batch_size, sequence length).&quot;</span>

        <span class="c1"># not allow to duplicate outputs when greedy decoding</span>
        <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">num_beams</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># no_beam_search greedy generation conditions</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">num_return_sequences</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="p">),</span> <span class="s2">&quot;Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences &gt; 1. Please set num_return_sequences = 1&quot;</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># beam_search greedy generation conditions</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">num_beams</span> <span class="o">&gt;=</span> <span class="n">num_return_sequences</span>
                <span class="p">),</span> <span class="s2">&quot;Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams &gt;= num_return_sequences&quot;</span>

        <span class="c1"># create attention mask if necessary</span>
        <span class="c1"># TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">numpy</span><span class="p">()):</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Setting `pad_token_id` to </span><span class="si">{}</span><span class="s2"> (first `eos_token_id`) to generate sequence&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>

        <span class="c1"># current position and vocab size</span>
        <span class="n">cur_len</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>

        <span class="c1"># set effective batch size and effective batch multiplier according to do_sample</span>
        <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
            <span class="n">effective_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
            <span class="n">effective_batch_mult</span> <span class="o">=</span> <span class="n">num_return_sequences</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">effective_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="n">effective_batch_mult</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># Expand input ids if num_beams &gt; 1 or num_return_sequences &gt; 1</span>
        <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">input_ids_len</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">effective_batch_mult</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">effective_batch_mult</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="p">(</span><span class="n">effective_batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># shape: (batch_size * num_return_sequences * num_beams, cur_len)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="n">effective_batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># shape: (batch_size * num_return_sequences * num_beams, cur_len)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span>

            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation&quot;</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;get_encoder&quot;</span><span class="p">),</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> should have a &#39;get_encoder&#39; function defined&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">),</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> should be a method&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">)</span>

            <span class="c1"># get encoder and store encoder outputs</span>
            <span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()</span>

            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

            <span class="c1"># create empty decoder_input_ids</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">effective_batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,)</span> <span class="o">*</span> <span class="n">decoder_start_token_id</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">min_length</span><span class="o">=</span><span class="n">min_length</span><span class="p">,</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="n">do_sample</span><span class="p">,</span>
                <span class="n">early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
                <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span>
                <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="n">no_repeat_ngram_size</span><span class="p">,</span>
                <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">decoder_start_token_id</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">effective_batch_size</span><span class="p">,</span>
                <span class="n">num_return_sequences</span><span class="o">=</span><span class="n">num_return_sequences</span><span class="p">,</span>
                <span class="n">length_penalty</span><span class="o">=</span><span class="n">length_penalty</span><span class="p">,</span>
                <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">encoder_outputs</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_no_beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">min_length</span><span class="o">=</span><span class="n">min_length</span><span class="p">,</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="n">do_sample</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
                <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span>
                <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="n">no_repeat_ngram_size</span><span class="p">,</span>
                <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">decoder_start_token_id</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">effective_batch_size</span><span class="p">,</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">encoder_outputs</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span></div>

    <span class="k">def</span> <span class="nf">_generate_no_beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">cur_len</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">,</span>
        <span class="n">min_length</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Generate sequences for each example without beam search (num_beams == 1).</span>
<span class="sd">            All returned sequence are generated independantly.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># length of generated sentences / unfinished sentences</span>
        <span class="n">unfinished_sents</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">sent_lengths</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">max_length</span>

        <span class="n">past</span> <span class="o">=</span> <span class="n">encoder_outputs</span>  <span class="c1"># defined for encoder-decoder models, None for decoder-only models</span>

        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">past</span><span class="o">=</span><span class="n">past</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># if model has past, then set the past variable to speed up decoding</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_do_output_past</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
                <span class="n">past</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)</span>
            <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">next_token_logits_penalties</span> <span class="o">=</span> <span class="n">_create_next_token_logits_penalties</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">repetition_penalty</span>
                <span class="p">)</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">next_token_logits_penalties</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># calculate a list of banned tokens to prevent repetitively generating the same ngrams</span>
                <span class="c1"># from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345</span>
                <span class="n">banned_tokens</span> <span class="o">=</span> <span class="n">calc_banned_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">)</span>
                <span class="c1"># create banned_tokens boolean mask</span>
                <span class="n">banned_tokens_indices_mask</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">banned_tokens_slice</span> <span class="ow">in</span> <span class="n">banned_tokens</span><span class="p">:</span>
                    <span class="n">banned_tokens_indices_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">banned_tokens_slice</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)]</span>
                    <span class="p">)</span>

                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span>
                    <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">banned_tokens_indices_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># set eos token prob to zero if min_length is not reached</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">min_length</span><span class="p">:</span>
                <span class="c1"># create eos_token_id boolean mask</span>
                <span class="n">is_token_logit_eos_token</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
                    <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="n">eos_token_id</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span>
                <span class="p">)</span>
                <span class="n">eos_token_indices_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">is_token_logit_eos_token</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">])</span>

                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span>
                    <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">eos_token_indices_mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                <span class="c1"># Temperature (higher temperature =&gt; more likely to sample low probability tokens)</span>
                <span class="k">if</span> <span class="n">temperature</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">next_token_logits</span> <span class="o">/</span> <span class="n">temperature</span>
                <span class="c1"># Top-p/top-k filtering</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">tf_top_k_top_p_filtering</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">)</span>
                <span class="c1"># Sample</span>
                <span class="n">next_token</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Greedy decoding</span>
                <span class="n">next_token</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

            <span class="c1"># update generations and finished sentences</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># pad finished sentences if eos_token_id exist</span>
                <span class="n">tokens_to_add</span> <span class="o">=</span> <span class="n">next_token</span> <span class="o">*</span> <span class="n">unfinished_sents</span> <span class="o">+</span> <span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">unfinished_sents</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tokens_to_add</span> <span class="o">=</span> <span class="n">next_token</span>

            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tokens_to_add</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">eos_in_sents</span> <span class="o">=</span> <span class="n">tokens_to_add</span> <span class="o">==</span> <span class="n">eos_token_id</span>
                <span class="c1"># if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length</span>
                <span class="n">is_sents_unfinished_and_token_to_add_is_eos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span>
                    <span class="n">unfinished_sents</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">eos_in_sents</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">sent_lengths</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">sent_lengths</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">is_sents_unfinished_and_token_to_add_is_eos</span><span class="p">)</span>
                    <span class="o">+</span> <span class="n">cur_len</span> <span class="o">*</span> <span class="n">is_sents_unfinished_and_token_to_add_is_eos</span>
                <span class="p">)</span>

                <span class="c1"># unfinished_sents is set to zero if eos in sentence</span>
                <span class="n">unfinished_sents</span> <span class="o">-=</span> <span class="n">is_sents_unfinished_and_token_to_add_is_eos</span>

            <span class="c1"># stop when there is a &lt;/s&gt; in each sentence, or if we exceed the maximul length</span>
            <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">unfinished_sents</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># extend attention_mask for new generated input if only decoder</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">shape_list</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
                <span class="p">)</span>

            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># if there are different sentences lengths in the batch, some batches have to be padded</span>
        <span class="n">min_sent_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span>
        <span class="n">max_sent_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">min_sent_length</span> <span class="o">!=</span> <span class="n">max_sent_length</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;`Pad_token_id` has to be defined if batches have different lengths&quot;</span>
            <span class="c1"># finished sents are filled with pad_token</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_sent_length</span><span class="o">.</span><span class="n">numpy</span><span class="p">()],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="n">pad_token_id</span>

            <span class="c1"># create length masks for tf.where operation</span>
            <span class="n">broad_casted_sent_lengths</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_sent_length</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">broad_casted_range</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="n">max_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">])</span>
            <span class="p">)</span>

            <span class="n">decoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">broad_casted_range</span> <span class="o">&lt;</span> <span class="n">broad_casted_sent_lengths</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">input_ids</span>

        <span class="k">return</span> <span class="n">decoded</span>

    <span class="k">def</span> <span class="nf">_generate_beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">cur_len</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">,</span>
        <span class="n">min_length</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="p">,</span>
        <span class="n">early_stopping</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">,</span>
        <span class="n">num_return_sequences</span><span class="p">,</span>
        <span class="n">length_penalty</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Generate sequences for each example with beam search.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># generated hypotheses</span>
        <span class="n">generated_hyps</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">BeamHypotheses</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">length_penalty</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times</span>
        <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">beam_scores_begin</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">beam_scores_end</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">beam_scores_begin</span><span class="p">,</span> <span class="n">beam_scores_end</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">beam_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,))</span>

        <span class="c1"># cache compute states</span>
        <span class="n">past</span> <span class="o">=</span> <span class="n">encoder_outputs</span>

        <span class="c1"># done sentences</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>

        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">past</span><span class="o">=</span><span class="n">past</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>  <span class="c1"># (batch_size * num_beams, cur_len, vocab_size)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

            <span class="c1"># if model has past, then set the past variable to speed up decoding</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_do_output_past</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
                <span class="n">past</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)</span>
            <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">next_token_logits_penalties</span> <span class="o">=</span> <span class="n">_create_next_token_logits_penalties</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">repetition_penalty</span>
                <span class="p">)</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">next_token_logits_penalties</span><span class="p">)</span>

            <span class="c1"># Temperature (higher temperature =&gt; more likely to sample low probability tokens)</span>
            <span class="k">if</span> <span class="n">temperature</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">next_token_logits</span> <span class="o">/</span> <span class="n">temperature</span>

            <span class="c1">#             calculate log softmax score</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

            <span class="c1"># set eos token prob to zero if min_length is not reached</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">min_length</span><span class="p">:</span>
                <span class="c1"># create eos_token_id boolean mask</span>
                <span class="n">num_batch_hypotheses</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span>

                <span class="n">is_token_logit_eos_token</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
                    <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="n">eos_token_id</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span>
                <span class="p">)</span>
                <span class="n">eos_token_indices_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">is_token_logit_eos_token</span><span class="p">,</span> <span class="p">[</span><span class="n">num_batch_hypotheses</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">])</span>

                <span class="n">scores</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">eos_token_indices_mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># calculate a list of banned tokens to prevent repetitively generating the same ngrams</span>
                <span class="c1"># from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345</span>
                <span class="n">num_batch_hypotheses</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span>
                <span class="n">banned_tokens</span> <span class="o">=</span> <span class="n">calc_banned_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">num_batch_hypotheses</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">)</span>
                <span class="c1"># create banned_tokens boolean mask</span>
                <span class="n">banned_tokens_indices_mask</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">banned_tokens_slice</span> <span class="ow">in</span> <span class="n">banned_tokens</span><span class="p">:</span>
                    <span class="n">banned_tokens_indices_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">banned_tokens_slice</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)]</span>
                    <span class="p">)</span>

                <span class="n">scores</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span>
                    <span class="n">scores</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">banned_tokens_indices_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="k">assert</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">==</span> <span class="p">[</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                <span class="n">_scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                    <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

                <span class="c1"># Top-p/top-k filtering</span>
                <span class="n">_scores</span> <span class="o">=</span> <span class="n">tf_top_k_top_p_filtering</span><span class="p">(</span>
                    <span class="n">_scores</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="mi">2</span>
                <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>
                <span class="c1"># Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search)</span>
                <span class="n">_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">))</span>

                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span>
                    <span class="n">_scores</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span>
                <span class="p">)</span>  <span class="c1"># (batch_size, 2 * num_beams)</span>
                <span class="c1"># Compute next scores</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">_scores</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, 2 * num_beams)</span>

                <span class="c1"># sort the sampled vector to make sure that the first num_beams samples are the best</span>
                <span class="n">next_scores_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">next_scores</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;DESCENDING&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_scores</span><span class="p">,</span> <span class="n">next_scores_indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, num_beams * 2)</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">,</span> <span class="n">next_scores_indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, num_beams * 2)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Add the log prob of the new beams to the log prob of the beginning of the sequence (sum of logs == log of the product)</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                    <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

                <span class="c1"># re-organize to group the beam together (we are keeping top hypothesis accross beams)</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="n">next_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># (batch_size, num_beams * vocab_size)</span>

                <span class="n">next_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span><span class="n">next_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">assert</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">next_scores</span><span class="p">)</span> <span class="o">==</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">)</span> <span class="o">==</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">]</span>

            <span class="c1"># next batch beam content</span>
            <span class="c1"># list of (batch_size * num_beams) tuple(next hypothesis score, next token, current position in the batch)</span>
            <span class="n">next_batch_beam</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># for each sentence</span>
            <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>

                <span class="k">if</span> <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]:</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="nb">len</span><span class="p">(</span><span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">num_beams</span>
                    <span class="p">),</span> <span class="s2">&quot;Batch can only be done if at least </span><span class="si">{}</span><span class="s2"> beams have been generated&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_beams</span><span class="p">)</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="p">),</span> <span class="s2">&quot;generated beams &gt;= num_beams -&gt; eos_token_id and pad_token have to be defined&quot;</span>
                    <span class="n">next_batch_beam</span><span class="o">.</span><span class="n">extend</span><span class="p">([(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">)</span>  <span class="c1"># pad the batch</span>
                    <span class="k">continue</span>

                <span class="c1"># next sentence beam content</span>
                <span class="n">next_sent_beam</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="c1"># next tokens for this sentence</span>
                <span class="k">for</span> <span class="n">beam_token_rank</span><span class="p">,</span> <span class="p">(</span><span class="n">beam_token_id</span><span class="p">,</span> <span class="n">beam_token_score</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="nb">zip</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">],</span> <span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span>
                <span class="p">):</span>

                    <span class="c1"># get beam and token IDs</span>
                    <span class="n">beam_id</span> <span class="o">=</span> <span class="n">beam_token_id</span> <span class="o">//</span> <span class="n">vocab_size</span>
                    <span class="n">token_id</span> <span class="o">=</span> <span class="n">beam_token_id</span> <span class="o">%</span> <span class="n">vocab_size</span>

                    <span class="n">effective_beam_id</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span>
                    <span class="c1"># add to generated hypotheses if end of sentence or last iteration</span>
                    <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">token_id</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="ow">is</span> <span class="n">eos_token_id</span><span class="p">:</span>
                        <span class="c1"># if beam_token does not belong to top num_beams tokens, it should not be added</span>
                        <span class="n">is_beam_token_worse_than_top_num_beams</span> <span class="o">=</span> <span class="n">beam_token_rank</span> <span class="o">&gt;=</span> <span class="n">num_beams</span>
                        <span class="k">if</span> <span class="n">is_beam_token_worse_than_top_num_beams</span><span class="p">:</span>
                            <span class="k">continue</span>
                        <span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="n">effective_beam_id</span><span class="p">]),</span> <span class="n">beam_token_score</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># add next predicted token if it is not eos_token</span>
                        <span class="n">next_sent_beam</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">beam_token_score</span><span class="p">,</span> <span class="n">token_id</span><span class="p">,</span> <span class="n">effective_beam_id</span><span class="p">))</span>

                    <span class="c1"># the beam for next step is full</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_sent_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_beams</span><span class="p">:</span>
                        <span class="k">break</span>

                <span class="c1"># if we are done with this sentence</span>
                <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="ow">or</span> <span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">is_done</span><span class="p">(</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="p">)</span>

                <span class="c1"># update next beam content</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_sent_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_beams</span><span class="p">,</span> <span class="s2">&quot;Beam should always be full&quot;</span>
                <span class="n">next_batch_beam</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">next_sent_beam</span><span class="p">)</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_batch_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># stop when we are done with each sentence</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">done</span><span class="p">):</span>
                <span class="k">break</span>

            <span class="c1"># sanity check / prepare next batch</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_batch_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_batch_beam</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">beam_tokens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_batch_beam</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_batch_beam</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

            <span class="c1"># re-order batch</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="p">:])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">beam_idx</span><span class="p">])</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">beam_tokens</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># re-order internal states</span>
            <span class="k">if</span> <span class="n">past</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">past</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reorder_cache</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">shape_list</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
                <span class="p">)</span>

            <span class="c1"># update current length</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># finalize all open beam hypotheses and end to generated hypotheses</span>
        <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># Add all open beam hypothesis to generated_hyps</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]:</span>
                <span class="k">continue</span>
            <span class="c1"># test that beam scores match previously calculated scores if not eos and batch_idx not done</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
                <span class="p">(</span><span class="n">token_id</span> <span class="o">%</span> <span class="n">vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">eos_token_id</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">next_tokens</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">assert</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span>
                    <span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="p">:</span><span class="n">num_beams</span><span class="p">]</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">beam_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">))[</span><span class="n">batch_idx</span><span class="p">]</span>
                <span class="p">),</span> <span class="s2">&quot;If batch_idx is not done, final next scores: </span><span class="si">{}</span><span class="s2"> have to equal to accumulated beam_scores: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">next_scores</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_beams</span><span class="p">][</span><span class="n">batch_idx</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">beam_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">))[</span><span class="n">batch_idx</span><span class="p">]</span>
                <span class="p">)</span>

            <span class="c1"># need to add best num_beams hypotheses to generated hyps</span>
            <span class="k">for</span> <span class="n">beam_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_beams</span><span class="p">):</span>
                <span class="n">effective_beam_id</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span>
                <span class="n">final_score</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="p">[</span><span class="n">effective_beam_id</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">final_tokens</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">effective_beam_id</span><span class="p">]</span>
                <span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">final_tokens</span><span class="p">,</span> <span class="n">final_score</span><span class="p">)</span>

        <span class="c1"># depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch</span>
        <span class="n">output_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="k">if</span> <span class="n">do_sample</span> <span class="k">else</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
        <span class="n">output_num_return_sequences_per_batch</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">do_sample</span> <span class="k">else</span> <span class="n">num_return_sequences</span>

        <span class="c1"># select the best hypotheses</span>
        <span class="n">sent_lengths_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># retrieve best hypotheses</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hypotheses</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">generated_hyps</span><span class="p">):</span>
            <span class="n">sorted_hyps</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">hypotheses</span><span class="o">.</span><span class="n">beams</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_num_return_sequences_per_batch</span><span class="p">):</span>
                <span class="n">best_hyp</span> <span class="o">=</span> <span class="n">sorted_hyps</span><span class="o">.</span><span class="n">pop</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">sent_lengths_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">best_hyp</span><span class="p">))</span>
                <span class="n">best</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_hyp</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">output_batch_size</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">best</span><span class="p">),</span> <span class="s2">&quot;Output batch size </span><span class="si">{}</span><span class="s2"> must match output beam hypotheses </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">output_batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">best</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">sent_lengths</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sent_lengths_list</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="c1"># shorter batches are filled with pad_token</span>
        <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;`Pad_token_id` has to be defined&quot;</span>
            <span class="n">sent_max_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
            <span class="n">decoded_list</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># fill with hypothesis and eos_token_id if necessary</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hypo</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best</span><span class="p">):</span>
                <span class="n">padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">sent_max_len</span> <span class="o">-</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">hypo</span><span class="p">)[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="n">pad_token_id</span>
                <span class="n">decoded_hypo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">hypo</span><span class="p">,</span> <span class="n">padding</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
                    <span class="n">decoded_hypo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span> <span class="o">==</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                        <span class="n">eos_token_id</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">sent_max_len</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                        <span class="n">decoded_hypo</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="n">decoded_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoded_hypo</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">decoded_list</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># none of the hypotheses have an eos_token</span>
            <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hypo</span><span class="p">)</span> <span class="o">==</span> <span class="n">max_length</span> <span class="k">for</span> <span class="n">hypo</span> <span class="ow">in</span> <span class="n">best</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">best</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">decoded</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past</span><span class="p">:</span>
            <span class="c1"># get the correct batch idx from layer past batch dim</span>
            <span class="c1"># batch dim of `past` and `mems` is at 2nd position</span>
            <span class="n">reordered_layer_past</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">layer_past</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">beam_idx</span><span class="p">]</span>
            <span class="n">reordered_layer_past</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">reordered_layer_past</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># check that shape matches</span>
            <span class="k">assert</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">reordered_layer_past</span><span class="p">)</span> <span class="o">==</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">layer_past</span><span class="p">)</span>
            <span class="n">reordered_past</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reordered_layer_past</span><span class="p">)</span>
        <span class="n">past</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">reordered_past</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">past</span></div>


<span class="k">def</span> <span class="nf">_create_next_token_logits_penalties</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="p">):</span>
    <span class="c1"># create logit penalties for already seen input_ids</span>
    <span class="n">token_penalties</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_list</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
    <span class="n">prev_input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">prev_input_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">):</span>
        <span class="n">logit_penalized</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">prev_input_id</span><span class="p">]</span>
        <span class="n">logit_penalties</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">logit_penalized</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># if previous logit score is &lt; 0 then multiply repetition penalty else divide</span>
        <span class="n">logit_penalties</span><span class="p">[</span><span class="n">logit_penalized</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">repetition_penalty</span>
        <span class="n">logit_penalties</span><span class="p">[</span><span class="n">logit_penalized</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">repetition_penalty</span>
        <span class="n">np</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">token_penalties</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">prev_input_id</span><span class="p">,</span> <span class="n">logit_penalties</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">token_penalties</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">calc_banned_tokens</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">,</span> <span class="n">num_hypos</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">):</span>
    <span class="c1"># Copied from fairseq for no_repeat_ngram in beam_search&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">no_repeat_ngram_size</span><span class="p">:</span>
        <span class="c1"># return no banned tokens if we haven&#39;t generated no_repeat_ngram_size tokens yet</span>
        <span class="k">return</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="n">generated_ngrams</span> <span class="o">=</span> <span class="p">[{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">):</span>
        <span class="n">gen_tokens</span> <span class="o">=</span> <span class="n">prev_input_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">generated_ngram</span> <span class="o">=</span> <span class="n">generated_ngrams</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">gen_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">no_repeat_ngram_size</span><span class="p">)]):</span>
            <span class="n">prev_ngram_tuple</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ngram</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">generated_ngram</span><span class="p">[</span><span class="n">prev_ngram_tuple</span><span class="p">]</span> <span class="o">=</span> <span class="n">generated_ngram</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prev_ngram_tuple</span><span class="p">,</span> <span class="p">[])</span> <span class="o">+</span> <span class="p">[</span><span class="n">ngram</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>

    <span class="k">def</span> <span class="nf">_get_generated_ngrams</span><span class="p">(</span><span class="n">hypo_idx</span><span class="p">):</span>
        <span class="c1"># Before decoding the next token, prevent decoding of ngrams that have already appeared</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">no_repeat_ngram_size</span>
        <span class="n">ngram_idx</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">cur_len</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">generated_ngrams</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ngram_idx</span><span class="p">,</span> <span class="p">[])</span>

    <span class="n">banned_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">_get_generated_ngrams</span><span class="p">(</span><span class="n">hypo_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">hypo_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">banned_tokens</span>


<span class="k">def</span> <span class="nf">tf_top_k_top_p_filtering</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">filter_value</span><span class="o">=-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Filter a distribution of logits using top-k and/or nucleus (top-p) filtering</span>
<span class="sd">        Args:</span>
<span class="sd">            logits: logits distribution shape (batch size, vocabulary size)</span>
<span class="sd">            if top_k &gt; 0: keep only top k tokens with highest probability (top-k filtering).</span>
<span class="sd">            if top_p &lt; 1.0: keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering).</span>
<span class="sd">                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)</span>
<span class="sd">            Make sure we keep at least min_tokens_to_keep per batch example in the output</span>
<span class="sd">        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logits_shape</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">top_k</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="p">),</span> <span class="n">logits_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Safety check</span>
        <span class="c1"># Remove all tokens with a probability less than the last token of the top-k</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">&lt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">indices_to_remove</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">top_p</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;DESCENDING&quot;</span><span class="p">)</span>
        <span class="n">sorted_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="n">logits</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># expects logits to be of dim (batch_size, vocab_size)</span>

        <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sorted_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Remove tokens with cumulative probability above the threshold (token with 0 are kept)</span>
        <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&gt;</span> <span class="n">top_p</span>

        <span class="k">if</span> <span class="n">min_tokens_to_keep</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)</span>
            <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sorted_indices_to_remove</span><span class="p">[:,</span> <span class="p">:</span><span class="n">min_tokens_to_keep</span><span class="p">]),</span>
                    <span class="n">sorted_indices_to_remove</span><span class="p">[:,</span> <span class="n">min_tokens_to_keep</span><span class="p">:],</span>
                <span class="p">],</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Shift the indices to the right to keep also the first token above the threshold</span>
        <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">sorted_indices_to_remove</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sorted_indices_to_remove</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">sorted_indices_to_remove</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># scatter sorted tensors to original indexing</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">scatter_values_on_batch_indices</span><span class="p">(</span><span class="n">sorted_indices_to_remove</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">indices_to_remove</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>


<span class="k">def</span> <span class="nf">scatter_values_on_batch_indices</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">batch_indices</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">batch_indices</span><span class="p">)</span>
    <span class="c1"># broadcast batch dim to shape</span>
    <span class="n">broad_casted_batch_dims</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># transform batch_indices to pair_indices</span>
    <span class="n">pair_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">broad_casted_batch_dims</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_indices</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])],</span> <span class="mi">0</span><span class="p">))</span>
    <span class="c1"># scatter values to pair indices</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">scatter_nd</span><span class="p">(</span><span class="n">pair_indices</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">set_tensor_by_indices_to_value</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="c1"># create value_tensor since tensor value assignment is not possible in TF</span>
    <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="o">+</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BeamHypotheses</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">length_penalty</span><span class="p">,</span> <span class="n">early_stopping</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize n-best list of hypotheses.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># ignoring bos_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span> <span class="o">=</span> <span class="n">early_stopping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">=</span> <span class="n">num_beams</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beams</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="mf">1e9</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Number of hypotheses in the list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyp</span><span class="p">,</span> <span class="n">sum_logprobs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a new hypothesis to the list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">sum_logprobs</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">hyp</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="ow">or</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">score</span><span class="p">,</span> <span class="n">hyp</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">:</span>
                <span class="n">sorted_scores</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([(</span><span class="n">s</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">)])</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">[</span><span class="n">sorted_scores</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="n">sorted_scores</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_done</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">best_sum_logprobs</span><span class="p">,</span> <span class="n">cur_len</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If there are enough hypotheses and that none of the hypotheses being generated</span>
<span class="sd">        can become better than the worst one in the heap, then we are done with this sentence.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cur_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cur_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span>
            <span class="n">cur_score</span> <span class="o">=</span> <span class="n">best_sum_logprobs</span> <span class="o">/</span> <span class="n">cur_len</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">&gt;=</span> <span class="n">cur_score</span>
            <span class="k">return</span> <span class="n">ret</span>


<span class="k">class</span> <span class="nc">TFConv1D</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; TFConv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)</span>
<span class="sd">            Basically works like a Linear layer but the weights are transposed</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nf</span> <span class="o">=</span> <span class="n">nf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nx</span> <span class="o">=</span> <span class="n">nx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">nx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">get_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">bz</span><span class="p">,</span> <span class="n">sl</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">x</span><span class="p">)[:</span><span class="mi">2</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nx</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">bz</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">TFSharedEmbeddings</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct shared token embeddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">initializer_range</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span> <span class="k">if</span> <span class="n">initializer_range</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">initializer_range</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build shared token embedding layer</span>
<span class="sd">        Shared weights logic adapted from</span>
<span class="sd">            https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">get_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;embedding&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get token embeddings of inputs.</span>
<span class="sd">        Args:</span>
<span class="sd">            inputs: list of three int64 tensors with shape [batch_size, length]: (input_ids, position_ids, token_type_ids)</span>
<span class="sd">            mode: string, a valid value is one of &quot;embedding&quot; and &quot;linear&quot;.</span>
<span class="sd">        Returns:</span>
<span class="sd">            outputs: (1) If mode == &quot;embedding&quot;, output embedding tensor, float32 with</span>
<span class="sd">                shape [batch_size, length, embedding_size]; (2) mode == &quot;linear&quot;, output</span>
<span class="sd">                linear tensor, float32 with shape [batch_size, length, vocab_size].</span>
<span class="sd">        Raises:</span>
<span class="sd">            ValueError: if mode is not valid.</span>

<span class="sd">        Shared weights logic adapted from</span>
<span class="sd">            https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;embedding&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mode </span><span class="si">{}</span><span class="s2"> is not valid.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Applies embedding based on inputs tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_linear</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes logits by running inputs through a linear layer.</span>
<span class="sd">            Args:</span>
<span class="sd">                inputs: A float32 tensor with shape [..., hidden_size]</span>
<span class="sd">            Returns:</span>
<span class="sd">                float32 tensor with shape [..., vocab_size].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">first_dims</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">first_dims</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">TFSequenceSummary</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Compute a single vector summary of a sequence hidden states according to various possibilities:</span>
<span class="sd">        Args of the config class:</span>
<span class="sd">            summary_type:</span>
<span class="sd">                - &#39;last&#39; =&gt; [default] take the last token hidden state (like XLNet)</span>
<span class="sd">                - &#39;first&#39; =&gt; take the first token hidden state (like Bert)</span>
<span class="sd">                - &#39;mean&#39; =&gt; take the mean of all tokens hidden states</span>
<span class="sd">                - &#39;cls_index&#39; =&gt; supply a Tensor of classification token position (GPT/GPT-2)</span>
<span class="sd">                - &#39;attn&#39; =&gt; Not implemented now, use multi-head attention</span>
<span class="sd">            summary_use_proj: Add a projection after the vector extraction</span>
<span class="sd">            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.</span>
<span class="sd">            summary_activation: &#39;tanh&#39; =&gt; add a tanh activation to the output, Other =&gt; no activation. Default</span>
<span class="sd">            summary_first_dropout: Add a dropout before the projection and activation</span>
<span class="sd">            summary_last_dropout: Add a dropout after the projection and activation</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_type</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_use_proj&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;last&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;attn&quot;</span><span class="p">:</span>
            <span class="c1"># We should use a standard multi-head attention module with absolute positional embedding for that.</span>
            <span class="c1"># Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276</span>
            <span class="c1"># We can probably just use the multi-head attention module of PyTorch &gt;=1.1.0</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">has_summary</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_use_proj&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_use_proj</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_summary</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_proj_to_labels&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_proj_to_labels</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">num_classes</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">num_classes</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">summary</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
                <span class="n">num_classes</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">get_initializer</span><span class="p">(</span><span class="n">initializer_range</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;summary&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">has_activation</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_activation&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_activation</span> <span class="o">==</span> <span class="s2">&quot;tanh&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_activation</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">tanh</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">has_first_dropout</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_first_dropout&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_first_dropout</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_first_dropout</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">first_dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">summary_first_dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">has_last_dropout</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_last_dropout&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_last_dropout</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_last_dropout</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">last_dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">summary_last_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; hidden_states: float Tensor in shape [bsz, seq_len, hidden_size], the hidden-states of the last layer.</span>
<span class="sd">            cls_index: [optional] position of the classification token if summary_type == &#39;cls_index&#39;,</span>
<span class="sd">                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.</span>
<span class="sd">                if summary_type == &#39;cls_index&#39; and cls_index is None:</span>
<span class="sd">                    we take the last token of the sequence as classification token</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs</span>
            <span class="n">cls_index</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">cls_index</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Too many inputs.&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span>
            <span class="n">cls_index</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;cls_index&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;last&quot;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;first&quot;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;cls_index&quot;</span><span class="p">:</span>
            <span class="n">hidden_shape</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># e.g. [batch, num choices, seq length, hidden dims]</span>
            <span class="k">if</span> <span class="n">cls_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cls_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span>
                    <span class="n">hidden_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">hidden_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="p">)</span>  <span class="c1"># A tensor full of shape [batch] or [batch, num choices] full of sequence length</span>
            <span class="n">cls_shape</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">cls_index</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cls_shape</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">cls_index</span> <span class="o">=</span> <span class="n">cls_index</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
            <span class="c1"># else:</span>
            <span class="c1"># cls_index = cls_index[..., tf.newaxis]</span>
            <span class="c1"># cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))</span>
            <span class="c1"># shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">cls_index</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
                <span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span>
            <span class="p">)</span>  <span class="c1"># shape of output: (batch, num choices, hidden_size)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;attn&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_first_dropout</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_dropout</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_summary</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_activation</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_last_dropout</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_dropout</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>


<span class="k">def</span> <span class="nf">shape_list</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deal with dynamic shape in tensorflow cleanly.&quot;&quot;&quot;</span>
    <span class="n">static</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">dynamic</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">dynamic</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">s</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">static</span><span class="p">)]</span>


<span class="k">def</span> <span class="nf">get_initializer</span><span class="p">(</span><span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a `tf.initializers.truncated_normal` with the given range.</span>
<span class="sd">    Args:</span>
<span class="sd">        initializer_range: float, initializer range for stddev.</span>
<span class="sd">    Returns:</span>
<span class="sd">        TruncatedNormal initializer with stddev = `initializer_range`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">TruncatedNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">initializer_range</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>