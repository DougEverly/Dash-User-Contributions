
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Models — transformers 2.6.0 documentation</title>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<script src="../_static/js/modernizr.min.js" type="text/javascript"></script>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/language_data.js"></script>
<script src="../_static/js/custom.js"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="tokenizer.html" rel="next" title="Tokenizer"/>
<link href="configuration.html" rel="prev" title="Configuration"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model upload and sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchscript.html">TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main classes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pretrainedmodel"><code class="docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfpretrainedmodel"><code class="docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a href="../index.html">Docs</a> »</li>
<li>Models</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/main_classes/model.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h1>
<p>The base class <code class="docutils literal notranslate"><span class="pre">PreTrainedModel</span></code> implements the common methods for loading/saving a model either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace’s AWS S3 repository).</p>
<p><code class="docutils literal notranslate"><span class="pre">PreTrainedModel</span></code> also implements a few methods which are common among all the models to:</p>
<ul class="simple">
<li><p>resize the input token embeddings when new tokens are added to the vocabulary</p></li>
<li><p>prune the attention heads of the model.</p></li>
</ul>
<div class="section" id="pretrainedmodel">
<h2><code class="docutils literal notranslate"><span class="pre">PreTrainedModel</span></code><a class="headerlink" href="#pretrainedmodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.PreTrainedModel"><a name="//apple_ref/cpp/Class/transformers.PreTrainedModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">PreTrainedModel</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">*inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all models.</p>
<p><a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a> takes care of storing the configuration of the models and handles methods for loading/downloading/saving models
as well as a few methods common to all models to (i) resize the input embeddings and (ii) prune heads in the self-attention heads.</p>
<dl>
<dt>Class attributes (overridden by derived classes):</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">config_class</span></code>: a class derived from <a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> to use as configuration class for this model architecture.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pretrained_model_archive_map</span></code>: a python <code class="docutils literal notranslate"><span class="pre">dict</span></code> of with <cite>short-cut-names</cite> (string) as keys and <cite>url</cite> (string) of associated pretrained weights as values.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">load_tf_weights</span></code>: a python <code class="docutils literal notranslate"><span class="pre">method</span></code> for loading a TensorFlow checkpoint in a PyTorch model, taking as arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: an instance of the relevant subclass of <a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">config</span></code>: an instance of the relevant subclass of <a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">path</span></code>: a path (string) to the TensorFlow checkpoint.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">base_model_prefix</span></code>: a string indicating the attribute associated to the base model in derived classes of the same architecture adding modules on top of the base model.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.dummy_inputs"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.dummy_inputs"></a>
<em class="property">property </em><code class="sig-name descname">dummy_inputs</code><a class="headerlink" href="#transformers.PreTrainedModel.dummy_inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>Dummy inputs to do a forward pass in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>torch.Tensor with dummy inputs</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.enforce_repetition_penalty_"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.enforce_repetition_penalty_"></a>
<code class="sig-name descname">enforce_repetition_penalty_</code><span class="sig-paren">(</span><em class="sig-param">lprobs</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">num_beams</em>, <em class="sig-param">prev_output_tokens</em>, <em class="sig-param">repetition_penalty</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.enforce_repetition_penalty_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.enforce_repetition_penalty_" title="Permalink to this definition">¶</a></dt>
<dd><p>repetition penalty (from CTRL paper <a class="reference external" href="https://arxiv.org/abs/1909.05858">https://arxiv.org/abs/1909.05858</a>).</p>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model_name_or_path</em>, <em class="sig-param">*model_args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate a pretrained pytorch model from a pre-trained model configuration.</p>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<p>The warning <code class="docutils literal notranslate"><span class="pre">Weights</span> <span class="pre">from</span> <span class="pre">XXX</span> <span class="pre">not</span> <span class="pre">initialized</span> <span class="pre">from</span> <span class="pre">pretrained</span> <span class="pre">model</span></code> means that the weights of XXX do not come pre-trained with the rest of the model.
It is up to you to train those weights with a downstream fine-tuning task.</p>
<p>The warning <code class="docutils literal notranslate"><span class="pre">Weights</span> <span class="pre">from</span> <span class="pre">XXX</span> <span class="pre">not</span> <span class="pre">used</span> <span class="pre">in</span> <span class="pre">YYY</span></code> means that the layer XXX is not used by YYY, therefore those weights are discarded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> – either:
- a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.
- a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.
- a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.
- a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
- None if you are both providing the configuration and state dictionary (resp. with keyword arguments <code class="docutils literal notranslate"><span class="pre">config</span></code> and <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>)</p></li>
<li><p><strong>model_args</strong> – (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</p></li>
<li><p><strong>config</strong> – <p>(<cite>optional</cite>) one of:
- an instance of a class derived from <a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, or
- a string valid as input to <a class="reference internal" href="configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</p></li>
<li><p>the model was saved using <a class="reference internal" href="#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</p></li>
<li><p>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> – (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p></li>
<li><p><strong>cache_dir</strong> – (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> – (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</p></li>
<li><p><strong>resume_download</strong> – (<cite>optional</cite>) boolean, default False:
Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</p></li>
<li><p><strong>proxies</strong> – (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}.
The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> – (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>kwargs</strong> – <p>(<cite>optional</cite>) Remaining dictionary of keyword arguments:
Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. <code class="docutils literal notranslate"><span class="pre">output_attention=True</span></code>). Behave differently depending on whether a <cite>config</cite> is provided or automatically loaded:</p>
<ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class initialization function (<a class="reference internal" href="configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># For example purposes. Not runnable.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/saved_model/'</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Update configuration during loading</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/my_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/my_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.generate"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.generate"></a>
<code class="sig-name descname">generate</code><span class="sig-paren">(</span><em class="sig-param">input_ids=None</em>, <em class="sig-param">max_length=None</em>, <em class="sig-param">min_length=None</em>, <em class="sig-param">do_sample=None</em>, <em class="sig-param">early_stopping=None</em>, <em class="sig-param">num_beams=None</em>, <em class="sig-param">temperature=None</em>, <em class="sig-param">top_k=None</em>, <em class="sig-param">top_p=None</em>, <em class="sig-param">repetition_penalty=None</em>, <em class="sig-param">bos_token_id=None</em>, <em class="sig-param">pad_token_id=None</em>, <em class="sig-param">eos_token_id=None</em>, <em class="sig-param">length_penalty=None</em>, <em class="sig-param">no_repeat_ngram_size=None</em>, <em class="sig-param">num_return_sequences=None</em>, <em class="sig-param">attention_mask=None</em>, <em class="sig-param">decoder_start_token_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.generate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.generate" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.</p>
<p>Adapted in part from <a class="reference external" href="https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529">Facebook’s XLM beam search code</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> – (<cite>optional</cite>) <cite>torch.LongTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>
The sequence used as a prompt for the generation. If <cite>None</cite> the method initializes
it as an empty <cite>torch.LongTensor</cite> of shape <cite>(1,)</cite>.</p></li>
<li><p><strong>max_length</strong> – (<cite>optional</cite>) int
The max length of the sequence to be generated.  Between <cite>min_length</cite> and infinity. Default to 20.</p></li>
<li><p><strong>min_length</strong> – (<cite>optional</cite>) int
The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.</p></li>
<li><p><strong>do_sample</strong> – (<cite>optional</cite>) bool
If set to <cite>False</cite> greedy decoding is used. Otherwise sampling is used. Defaults to <cite>False</cite> as defined in <cite>configuration_utils.PretrainedConfig</cite>.</p></li>
<li><p><strong>early_stopping</strong> – (<cite>optional</cite>) bool
if set to <cite>True</cite> beam search is stopped when at least <cite>num_beams</cite> sentences finished per batch. Defaults to <cite>False</cite> as defined in <cite>configuration_utils.PretrainedConfig</cite>.</p></li>
<li><p><strong>num_beams</strong> – (<cite>optional</cite>) int
Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.</p></li>
<li><p><strong>temperature</strong> – (<cite>optional</cite>) float
The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.</p></li>
<li><p><strong>top_k</strong> – (<cite>optional</cite>) int
The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.</p></li>
<li><p><strong>top_p</strong> – (<cite>optional</cite>) float
The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.</p></li>
<li><p><strong>repetition_penalty</strong> – (<cite>optional</cite>) float
The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.</p></li>
<li><p><strong>pad_token_id</strong> – (<cite>optional</cite>) int
Padding token. Default to specicic model pad_token_id or None if it does not exist.</p></li>
<li><p><strong>bos_token_id</strong> – (<cite>optional</cite>) int
BOS token. Defaults to bos_token_id as defined in the models config.</p></li>
<li><p><strong>pad_token_id</strong> – (<cite>optional</cite>) int
Pad token. Defaults to pad_token_id as defined in the models config.</p></li>
<li><p><strong>eos_token_ids</strong> – (<cite>optional</cite>) int or list of int
End of sequence token or list of tokens to stop the generation. Default to eos_token_ids as defined in the models config.</p></li>
<li><p><strong>length_penalty</strong> – (<cite>optional</cite>) float
Exponential penalty to the length. Default to 1.</p></li>
<li><p><strong>no_repeat_ngram_size</strong> – (<cite>optional</cite>) int
If set to int &gt; 0, all ngrams of size <cite>no_repeat_ngram_size</cite> can only occur once.</p></li>
<li><p><strong>num_return_sequences</strong> – (<cite>optional</cite>) int
The number of independently computed returned sequences for each element in the batch. Default to 1.</p></li>
<li><p><strong>attention_mask</strong> (<cite>optional</cite>) – <cite>torch.LongTensor</cite> of same shape as <cite>input_ids</cite>
Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.
Defaults to <cite>None</cite>.</p></li>
<li><p><strong>are attention masks?_</strong> (<em>What</em>) – </p></li>
<li><p><strong>decoder_start_token_id=None</strong> – (<cite>optional</cite>) int
If an encoder-decoder model starts decoding with a different token than BOS.
Defaults to <cite>None</cite> and is changed to <cite>BOS</cite> later.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt><cite>torch.LongTensor</cite> of shape <cite>(batch_size * num_return_sequences, sequence_length)</cite></dt><dd><p>sequence_length is either equal to max_length or shorter if all batches finished early due to the <cite>eos_token_id</cite></p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>  <span class="c1"># do greedy decoding</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Generated: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'openai-gpt'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'openai-gpt'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">'The dog'</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>  <span class="c1"># generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="c1">#  3 output sequences were generated</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Generated </span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">'The dog'</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># 3 generate sequences using by sampling</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="c1">#  3 output sequences were generated</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Generated </span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'ctrl'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'ctrl'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">'Legal My neighbor is'</span>  <span class="c1"># "Legal" is one of the control codes for ctrl</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>  <span class="c1"># generate sequences</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Generated: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.get_input_embeddings"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.get_input_embeddings"></a>
<code class="sig-name descname">get_input_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.get_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.get_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s input embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping vocabulary to hidden states.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.get_output_embeddings"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.get_output_embeddings"></a>
<code class="sig-name descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.get_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.get_output_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s output embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping hidden states to vocabulary.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.init_weights"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.init_weights"></a>
<code class="sig-name descname">init_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.init_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.init_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize and prunes weights if needed.</p>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.prune_heads"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.prune_heads"></a>
<code class="sig-name descname">prune_heads</code><span class="sig-paren">(</span><em class="sig-param">heads_to_prune</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.prune_heads"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.prune_heads" title="Permalink to this definition">¶</a></dt>
<dd><p>Prunes heads of the base model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>heads_to_prune</strong> – dict with keys being selected layer indices (<cite>int</cite>) and associated values being the list of heads to prune in said layer (list of <cite>int</cite>).</p></li>
<li><p><strong>{1</strong> (<em>E.g.</em>) – [0, 2], 2: [2, 3]} will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.resize_token_embeddings"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.resize_token_embeddings"></a>
<code class="sig-name descname">resize_token_embeddings</code><span class="sig-paren">(</span><em class="sig-param">new_num_tokens=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.resize_token_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.resize_token_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.
Take care of tying weights embeddings afterwards if the model class has a <cite>tie_weights()</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_num_tokens</strong> – (<cite>optional</cite>) int:
New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end.
If not provided or None: does nothing and just returns a pointer to the input tokens <code class="docutils literal notranslate"><span class="pre">torch.nn.Embeddings</span></code> Module of the model.</p>
</dd>
</dl>
<dl class="simple">
<dt>Return: <code class="docutils literal notranslate"><span class="pre">torch.nn.Embeddings</span></code></dt><dd><p>Pointer to the input tokens Embeddings Module of the model</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.save_pretrained"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.save_pretrained"></a>
<code class="sig-name descname">save_pretrained</code><span class="sig-paren">(</span><em class="sig-param">save_directory</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.save_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.save_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a model and its configuration file to a directory, so that it
can be re-loaded using the <cite>:func:`~transformers.PreTrainedModel.from_pretrained`</cite> class method.</p>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.set_input_embeddings"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.set_input_embeddings"></a>
<code class="sig-name descname">set_input_embeddings</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.set_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.set_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Set model’s input embeddings</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) – A module mapping vocabulary to hidden states.</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.PreTrainedModel.tie_weights"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.tie_weights"></a>
<code class="sig-name descname">tie_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.tie_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.tie_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Tie the weights between the input embeddings and the output embeddings.
If the <cite>torchscript</cite> flag is set in the configuration, can’t handle parameter sharing so we are cloning
the weights instead.</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfpretrainedmodel">
<h2><code class="docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code><a class="headerlink" href="#tfpretrainedmodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.TFPreTrainedModel"><a name="//apple_ref/cpp/Class/transformers.TFPreTrainedModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFPreTrainedModel</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">*inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all TF models.</p>
<p><a class="reference internal" href="#transformers.TFPreTrainedModel" title="transformers.TFPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a> takes care of storing the configuration of the models and handles methods for loading/downloading/saving models
as well as a few methods common to all models to (i) resize the input embeddings and (ii) prune heads in the self-attention heads.</p>
<dl>
<dt>Class attributes (overridden by derived classes):</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">config_class</span></code>: a class derived from <a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> to use as configuration class for this model architecture.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pretrained_model_archive_map</span></code>: a python <code class="docutils literal notranslate"><span class="pre">dict</span></code> of with <cite>short-cut-names</cite> (string) as keys and <cite>url</cite> (string) of associated pretrained weights as values.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">load_tf_weights</span></code>: a python <code class="docutils literal notranslate"><span class="pre">method</span></code> for loading a TensorFlow checkpoint in a PyTorch model, taking as arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: an instance of the relevant subclass of <a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">config</span></code>: an instance of the relevant subclass of <a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">path</span></code>: a path (string) to the TensorFlow checkpoint.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">base_model_prefix</span></code>: a string indicating the attribute associated to the base model in derived classes of the same architecture adding modules on top of the base model.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="transformers.TFPreTrainedModel.dummy_inputs"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.dummy_inputs"></a>
<em class="property">property </em><code class="sig-name descname">dummy_inputs</code><a class="headerlink" href="#transformers.TFPreTrainedModel.dummy_inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>Dummy inputs to build the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>tf.Tensor with dummy inputs</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.TFPreTrainedModel.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model_name_or_path</em>, <em class="sig-param">*model_args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate a pretrained TF 2.0 model from a pre-trained model configuration.</p>
<p>The warning <code class="docutils literal notranslate"><span class="pre">Weights</span> <span class="pre">from</span> <span class="pre">XXX</span> <span class="pre">not</span> <span class="pre">initialized</span> <span class="pre">from</span> <span class="pre">pretrained</span> <span class="pre">model</span></code> means that the weights of XXX do not come pre-trained with the rest of the model.
It is up to you to train those weights with a downstream fine-tuning task.</p>
<p>The warning <code class="docutils literal notranslate"><span class="pre">Weights</span> <span class="pre">from</span> <span class="pre">XXX</span> <span class="pre">not</span> <span class="pre">used</span> <span class="pre">in</span> <span class="pre">YYY</span></code> means that the layer XXX is not used by YYY, therefore those weights are discarded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> – <p>either:</p>
<ul>
<li><p>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>a path or url to a <cite>PyTorch state_dict save file</cite> (e.g. <cite>./pt_model/pytorch_model.bin</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch checkpoint in a TensorFlow model using the provided conversion scripts and loading the TensorFlow model afterwards.</p></li>
</ul>
</p></li>
<li><p><strong>model_args</strong> – (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</p></li>
<li><p><strong>config</strong> – <dl class="simple">
<dt>(<cite>optional</cite>) one of:</dt><dd><ul>
<li><p>an instance of a class derived from <a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, or</p></li>
<li><p>a string valid as input to <a class="reference internal" href="configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a></p></li>
</ul>
</dd>
</dl>
<p>Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li><p>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</p></li>
<li><p>the model was saved using <a class="reference internal" href="#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</p></li>
<li><p>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</p></li>
<li><p><strong>from_pt</strong> – (<cite>optional</cite>) boolean, default False:
Load the model weights from a PyTorch state_dict save file (see docstring of pretrained_model_name_or_path argument).</p></li>
<li><p><strong>cache_dir</strong> – (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> – (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</p></li>
<li><p><strong>resume_download</strong> – (<cite>optional</cite>) boolean, default False:
Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</p></li>
<li><p><strong>proxies</strong> – (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}.
The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> – (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>kwargs</strong> – <p>(<cite>optional</cite>) Remaining dictionary of keyword arguments:
Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. <code class="docutils literal notranslate"><span class="pre">output_attention=True</span></code>). Behave differently depending on whether a <cite>config</cite> is provided or automatically loaded:</p>
<ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class initialization function (<a class="reference internal" href="configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># For example purposes. Not runnable.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/saved_model/'</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Update configuration during loading</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/my_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/my_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.TFPreTrainedModel.generate"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.generate"></a>
<code class="sig-name descname">generate</code><span class="sig-paren">(</span><em class="sig-param">input_ids=None</em>, <em class="sig-param">max_length=None</em>, <em class="sig-param">min_length=None</em>, <em class="sig-param">do_sample=None</em>, <em class="sig-param">early_stopping=None</em>, <em class="sig-param">num_beams=None</em>, <em class="sig-param">temperature=None</em>, <em class="sig-param">top_k=None</em>, <em class="sig-param">top_p=None</em>, <em class="sig-param">repetition_penalty=None</em>, <em class="sig-param">bos_token_id=None</em>, <em class="sig-param">pad_token_id=None</em>, <em class="sig-param">eos_token_id=None</em>, <em class="sig-param">length_penalty=None</em>, <em class="sig-param">no_repeat_ngram_size=None</em>, <em class="sig-param">num_return_sequences=None</em>, <em class="sig-param">attention_mask=None</em>, <em class="sig-param">decoder_start_token_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.generate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.generate" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates sequences for models with a LM head. The method currently supports greedy or penalized greedy decoding, sampling with top-k or nucleus sampling
and beam-search.</p>
<p>Adapted in part from <a class="reference external" href="https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529">Facebook’s XLM beam search code</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> – (<cite>optional</cite>) <cite>tf.Tensor</cite> of <cite>dtype=tf.int32</cite> of shape <cite>(batch_size, sequence_length)</cite>
The sequence used as a prompt for the generation. If <cite>None</cite> the method initializes
it as an empty <cite>torch.LongTensor</cite> of shape <cite>(1,)</cite>.</p></li>
<li><p><strong>max_length</strong> – (<cite>optional</cite>) int
The max length of the sequence to be generated.  Between 1 and infinity. Default to 20.</p></li>
<li><p><strong>min_length</strong> – (<cite>optional</cite>) int
The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.</p></li>
<li><p><strong>do_sample</strong> – (<cite>optional</cite>) bool
If set to <cite>False</cite> greedy decoding is used. Otherwise sampling is used. Defaults to <cite>False</cite> as defined in <cite>configuration_utils.PretrainedConfig</cite>.</p></li>
<li><p><strong>early_stopping</strong> – (<cite>optional</cite>) bool
if set to <cite>True</cite> beam search is stopped when at least <cite>num_beams</cite> sentences finished per batch. Defaults to <cite>False</cite> as defined in <cite>configuration_utils.PretrainedConfig</cite>.</p></li>
<li><p><strong>num_beams</strong> – (<cite>optional</cite>) int
Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.</p></li>
<li><p><strong>temperature</strong> – (<cite>optional</cite>) float
The value used to module the next token probabilities. Must be strictely positive. Default to 1.0.</p></li>
<li><p><strong>top_k</strong> – (<cite>optional</cite>) int
The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.</p></li>
<li><p><strong>top_p</strong> – (<cite>optional</cite>) float
The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.</p></li>
<li><p><strong>repetition_penalty</strong> – (<cite>optional</cite>) float
The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.</p></li>
<li><p><strong>bos_token_id</strong> – (<cite>optional</cite>) int
Beginning of sentence token if no prompt is provided. Default to specicic model bos_token_id or None if it does not exist.</p></li>
<li><p><strong>pad_token_id</strong> – (<cite>optional</cite>) int
Pad token. Defaults to pad_token_id as defined in the models config.</p></li>
<li><p><strong>eos_token_ids</strong> – (<cite>optional</cite>) int or list of int
End of sequence token or list of tokens to stop the generation. Default to 0.</p></li>
<li><p><strong>length_penalty</strong> – (<cite>optional</cite>) float
Exponential penalty to the length. Default to 1.</p></li>
<li><p><strong>no_repeat_ngram_size</strong> – (<cite>optional</cite>) int
If set to int &gt; 0, all ngrams of size <cite>no_repeat_ngram_size</cite> can only occur once.</p></li>
<li><p><strong>num_return_sequences</strong> – (<cite>optional</cite>) int
The number of independently computed returned sequences for each element in the batch. Default to 1.</p></li>
<li><p><strong>attention_mask</strong> (<cite>optional</cite>) – <p><cite>tf.Tensor</cite> with <cite>dtype=tf.int32</cite> of same shape as <cite>input_ids</cite>
Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.
Defaults to <cite>None</cite>.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>decoder_start_token_id=None</strong> – (<cite>optional</cite>) int
If an encoder-decoder model starts decoding with a different token than BOS.
Defaults to <cite>None</cite> and is changed to <cite>BOS</cite> later.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt><cite>tf.Tensor</cite> of <cite>dtype=tf.int32</cite> shape <cite>(batch_size * num_return_sequences, sequence_length)</cite></dt><dd><p>sequence_length is either equal to max_length or shorter if all batches finished early due to the <cite>eos_token_id</cite></p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>  <span class="c1"># do greedy decoding</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Generated: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'openai-gpt'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'openai-gpt'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">'The dog'</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'tf'</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>  <span class="c1"># generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="c1">#  3 output sequences were generated</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Generated </span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">'The dog'</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'tf'</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># 3 generate sequences using by sampling</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="c1">#  3 output sequences were generated</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Generated </span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'ctrl'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'ctrl'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">'Legal My neighbor is'</span>  <span class="c1"># "Legal" is one of the control codes for ctrl</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'tf'</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>  <span class="c1"># generate sequences</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Generated: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.TFPreTrainedModel.get_input_embeddings"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.get_input_embeddings"></a>
<code class="sig-name descname">get_input_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.get_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.get_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s input embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping vocabulary to hidden states.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.TFPreTrainedModel.get_output_embeddings"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.get_output_embeddings"></a>
<code class="sig-name descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.get_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.get_output_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s output embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping hidden states to vocabulary.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.TFPreTrainedModel.prune_heads"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.prune_heads"></a>
<code class="sig-name descname">prune_heads</code><span class="sig-paren">(</span><em class="sig-param">heads_to_prune</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.prune_heads"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.prune_heads" title="Permalink to this definition">¶</a></dt>
<dd><p>Prunes heads of the base model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>heads_to_prune</strong> – dict with keys being selected layer indices (<cite>int</cite>) and associated values being the list of heads to prune in said layer (list of <cite>int</cite>).</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.TFPreTrainedModel.resize_token_embeddings"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.resize_token_embeddings"></a>
<code class="sig-name descname">resize_token_embeddings</code><span class="sig-paren">(</span><em class="sig-param">new_num_tokens=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.resize_token_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.resize_token_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.
Take care of tying weights embeddings afterwards if the model class has a <cite>tie_weights()</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_num_tokens</strong> – (<cite>optional</cite>) int:
New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end.
If not provided or None: does nothing and just returns a pointer to the input tokens <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> Module of the model.</p>
</dd>
</dl>
<dl class="simple">
<dt>Return: <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code></dt><dd><p>Pointer to the input tokens Embeddings Module of the model</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="transformers.TFPreTrainedModel.save_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.save_pretrained"></a>
<code class="sig-name descname">save_pretrained</code><span class="sig-paren">(</span><em class="sig-param">save_directory</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.save_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.save_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a model and its configuration file to a directory, so that it
can be re-loaded using the <a class="reference internal" href="#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method.</p>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="tokenizer.html" rel="next" title="Tokenizer">Next <span class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="configuration.html" rel="prev" title="Configuration"><span class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        © Copyright 2020, huggingface

    </p>
</div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    ga('send', 'pageview');
    </script>
</body>
</html>